import warnings
import traceback

from benchmark_gguf.loader_gguf import load_gguf_model
from benchmark_gguf.suite_gguf import EnhancedBenchmarkSuiteGGUF
from benchmark_gguf.utils_gguf import get_model_hash
from config.test_cases import MMLU_TEST_DIR, PERPLEXITY_CATEGORIES, TRADITIONAL_GENERATION_TASKS

warnings.filterwarnings("ignore")

# === C·∫§U H√åNH CHO MODEL GGUF ===
# Thay ƒë·ªïi repo ID v√† t√™n file cho ph√π h·ª£p v·ªõi model b·∫°n mu·ªën test
MODEL_REPO_ID = "unsloth/Llama-3.1-8B-Instruct-GGUF"
MODEL_FILE = "unsloth/Llama-3.1-8B-Instruct-GGUF" 
MODEL_CACHE_DIR = "./models"

# C·∫•u h√¨nh cho Llama.cpp
GGUF_CONFIG = {
    "n_gpu_layers": -1,  # Offload t·∫•t c·∫£ c√°c layer l√™n GPU. ƒê·∫∑t l√† 0 n·∫øu ch·ªâ d√πng CPU.
    "n_ctx": 4096,       # K√≠ch th∆∞·ªõc ng·ªØ c·∫£nh
    "verbose": False     # T·∫Øt log chi ti·∫øt c·ªßa llama.cpp
}

def main():
    run_name = MODEL_FILE.replace(".gguf", "")
    print(f"üîç Starting Benchmark for GGUF Model: {MODEL_REPO_ID}/{MODEL_FILE}")
    print(f"üöÄ Run Name: {run_name}")
    print("=" * 80)
    
    try:
        # 1. X√°c ƒë·ªãnh model v√† hash
        model_config = {'repo_id': MODEL_REPO_ID, 'file': MODEL_FILE, 'config': GGUF_CONFIG}
        model_hash = get_model_hash(MODEL_REPO_ID, model_config)
        print(f"üîë Model Config Hash: {model_hash}")
        
        # 2. T·∫£i model GGUF
        llm = load_gguf_model(
            repo_id=MODEL_REPO_ID,
            model_file=MODEL_FILE,
            model_cache_dir=MODEL_CACHE_DIR,
            **GGUF_CONFIG
        )
        
        if llm is None:
            print("‚ùå Failed to load GGUF model!"); return
        
        print("‚úÖ GGUF model is loaded and ready for benchmarking!")
        
        # 3. Kh·ªüi t·∫°o v√† ch·∫°y b·ªô benchmark
        benchmark_suite = EnhancedBenchmarkSuiteGGUF(
            llm=llm,
            model_id=f"{MODEL_REPO_ID}/{MODEL_FILE}",
            run_name=run_name,
            model_hash=model_hash
        )
        
        # Truy·ªÅn c√°c test case t·ª´ file config v√†o
        benchmark_suite.run_full_benchmark(
            mmlu_dir=MMLU_TEST_DIR,
            PERPLEXITY_CATEGORIES=PERPLEXITY_CATEGORIES,
            traditional_tasks=TRADITIONAL_GENERATION_TASKS
        )
        
        print(f"\nüéä Benchmark for {MODEL_FILE} completed successfully!")
        
    except Exception as e:
        print(f"‚ùå An unexpected error occurred: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()