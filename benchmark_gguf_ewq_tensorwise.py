import warnings
import traceback
import os

from llama_cpp import Llama 
from benchmark_gguf.suite_gguf import EnhancedBenchmarkSuiteGGUF
from config.test_cases import MMLU_TEST_DIR, PERPLEXITY_CATEGORIES, TRADITIONAL_GENERATION_TASKS
from benchmark_gguf.utils_gguf import get_model_hash

warnings.filterwarnings("ignore")

LOCAL_MODEL_PATH = "./models/Qwen3-8B-gguf-ewq-tensorwise.gguf" 
GGUF_CONFIG = {
    "logits_all":True,
    "n_gpu_layers": -1,  # Offload táº¥t cáº£ cÃ¡c layer lÃªn GPU. Äáº·t lÃ  0 náº¿u chá»‰ dÃ¹ng CPU.
    "n_ctx": 8192,       # KÃ­ch thÆ°á»›c ngá»¯ cáº£nh
    "verbose": False     # Táº¯t log chi tiáº¿t cá»§a llama.cpp
}


def main():
    # Láº¥y tÃªn file tá»« Ä‘Æ°á»ng dáº«n Ä‘á»ƒ Ä‘áº·t tÃªn cho láº§n cháº¡y benchmark
    model_filename = os.path.basename(LOCAL_MODEL_PATH)
    run_name = model_filename.replace(".gguf", "")

    print(f"ğŸ” Starting Benchmark for LOCAL GGUF Model: {LOCAL_MODEL_PATH}")
    print(f"ğŸš€ Run Name: {run_name}")
    print("=" * 80)
    
    try:
        # 1. XÃ¡c Ä‘á»‹nh model vÃ  Ä‘á»‹nh danh
        # Äá»‘i vá»›i model offline, Ä‘Æ°á»ng dáº«n file chÃ­nh lÃ  Ä‘á»‹nh danh duy nháº¥t
        model_identifier = LOCAL_MODEL_PATH
        print(f"ğŸ”‘ Model Identifier: {model_identifier}")
        
        # 2. Táº£i model GGUF trá»±c tiáº¿p tá»« file
        # ÄÃ¢y lÃ  thay Ä‘á»•i quan trá»ng nháº¥t!
        print(f"â³ Loading GGUF model from local path: {LOCAL_MODEL_PATH}")
        llm = Llama(
            model_path=LOCAL_MODEL_PATH,
            **GGUF_CONFIG
        )
        
        print("âœ… GGUF model is loaded and ready for benchmarking!")
        
        # 3. Khá»Ÿi táº¡o vÃ  cháº¡y bá»™ benchmark
        benchmark_suite = EnhancedBenchmarkSuiteGGUF(
            llm=llm,
            model_id=model_identifier,
            run_name=run_name,
            model_hash=get_model_hash(run_name, GGUF_CONFIG)  # Sá»­ dá»¥ng run_name vÃ  GGUF_CONFIG Ä‘á»ƒ táº¡o hash duy nháº¥t
        )
        
        # Truyá»n cÃ¡c test case tá»« file config vÃ o
        benchmark_suite.run_full_benchmark(
            mmlu_dir=MMLU_TEST_DIR,
            PERPLEXITY_CATEGORIES=PERPLEXITY_CATEGORIES,
            traditional_tasks=TRADITIONAL_GENERATION_TASKS
        )
        
        print(f"\nğŸŠ Benchmark for {model_filename} completed successfully!")
        
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()