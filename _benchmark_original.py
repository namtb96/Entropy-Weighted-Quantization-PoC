#!/usr/bin/env python3
"""
Original Model Benchmark System - WITHOUT EWQ Quantization

Script nÃ y dÃ¹ng Ä‘á»ƒ benchmark model gá»‘c (khÃ´ng lÆ°á»£ng tá»­ hÃ³a) Ä‘á»ƒ so sÃ¡nh vá»›i káº¿t quáº£ EWQ.
Bao gá»“m táº¥t cáº£ cÃ¡c bÃ i test: MMLU, Perplexity vÃ  Generation tasks.

Quy trÃ¬nh:
1. Táº£i model gá»‘c lÃªn GPU vá»›i precision cao nháº¥t cÃ³ thá»ƒ.
2. Cháº¡y toÃ n bá»™ bá»™ benchmark vá»›i Ä‘áº§y Ä‘á»§ logging.
3. LÆ°u káº¿t quáº£ Ä‘á»ƒ so sÃ¡nh vá»›i EWQ benchmark.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import psutil
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import gc
import os
import warnings
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

warnings.filterwarnings("ignore")

# === Cáº¥u hÃ¬nh ===
MODEL_ID = "unsloth/Meta-Llama-3.1-8B-Instruct"
MODEL_CACHE_DIR = "./models"

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# === MMLU Test Questions (Sample subset for efficiency) ===
MMLU_QUESTIONS = {
    "abstract_algebra": [
        {
            "question": "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.",
            "choices": ["0", "4", "2", "6"],
            "answer": 1
        },
        {
            "question": "Let p = (1, 2, 5, 4)(2, 3) in S_5. Find the index of <p> in S_5.",
            "choices": ["8", "2", "24", "120"],
            "answer": 2
        }
    ],
    "anatomy": [
        {
            "question": "Which of the following is the body cavity that contains the pituitary gland?",
            "choices": ["Abdominal", "Cranial", "Pleural", "Spinal"],
            "answer": 1
        },
        {
            "question": "What is the embryological origin of the hyoid bone?",
            "choices": ["The first pharyngeal arch", "The first and second pharyngeal arches", "The second pharyngeal arch", "The second and third pharyngeal arches"],
            "answer": 3
        }
    ],
    "astronomy": [
        {
            "question": "How long does it take for light to travel from the Sun to the Earth?",
            "choices": ["8 minutes", "1 hour", "1 day", "1 year"],
            "answer": 0
        },
        {
            "question": "Where do most short-period comets come from and how do we know?",
            "choices": ["The Kuiper belt; short period comets tend to be in the plane of the solar system just like the Kuiper belt.", "The Kuiper belt; short period comets tend to come from random directions indicating a spherical distribution of comets called the Kuiper belt.", "The Oort cloud; short period comets tend to be in the plane of the solar system just like the Oort cloud.", "The Oort cloud; short period comets tend to come from random directions indicating a spherical distribution of comets called the Oort cloud."],
            "answer": 0
        }
    ],
    "business_ethics": [
        {
            "question": "According to Kant's categorical imperative, we should:",
            "choices": ["Act only according to that maxim whereby you can at the same time will that it should become a universal law.", "Act to maximize happiness for the greatest number of people.", "Act only according to that maxim which you can at the same time will to be a universal law of nature.", "Act to benefit yourself, as long as you don't harm others."],
            "answer": 0
        },
        {
            "question": "What is the difference between a stakeholder and a shareholder?",
            "choices": ["Stakeholders own shares in the company, while shareholders have an interest in the company.", "Stakeholders have an interest in the company, while shareholders own shares in the company.", "Stakeholders and shareholders are the same thing.", "Stakeholders are employees, while shareholders are customers."],
            "answer": 1
        }
    ],
    "clinical_knowledge": [
        {
            "question": "Glycolysis is the name given to the pathway involving the conversion of:",
            "choices": ["glycogen to glucose-1-phosphate.", "glycogen or starch to fructose.", "glycogen or starch to glucose or glucose-1-phosphate.", "glucose to pyruvate or lactate."],
            "answer": 3
        },
        {
            "question": "A patient has been on the operating table for four hours. How long may it take for any pressure damage to be reversible?",
            "choices": ["30 minutes.", "2 hours.", "12 hours.", "24 hours."],
            "answer": 2
        }
    ]
}

# === Perplexity Test Passages ===
PERPLEXITY_PASSAGES = [
    "The quick brown fox jumps over the lazy dog. This pangram contains every letter of the English alphabet at least once.",
    "In the beginning was the Word, and the Word was with God, and the Word was God. He was in the beginning with God.",
    "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from and make predictions on data.",
    "The mitochondria is the powerhouse of the cell, responsible for producing ATP through cellular respiration.",
    "Climate change refers to long-term shifts in temperatures and weather patterns, primarily caused by human activities since the 1800s.",
    "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles.",
    "The Internet has revolutionized communication, commerce, and access to information, connecting billions of people worldwide.",
    "Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create oxygen and energy in the form of sugar."
]

def get_model_hash(model_id: str) -> str:
    """Táº¡o hash cho model gá»‘c (khÃ´ng quantization)."""
    config_str = f"{model_id}-original-no-quantization"
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]

def load_original_model(model_id: str) -> Tuple[Optional[nn.Module], Optional[AutoTokenizer]]:
    """Táº£i model gá»‘c vá»›i precision cao nháº¥t cÃ³ thá»ƒ."""
    print("  ğŸ“¥ Loading original model to GPU...")
    
    # Kiá»ƒm tra VRAM available
    if torch.cuda.is_available():
        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  ğŸ’¾ Available VRAM: {total_vram:.2f} GB")
        
        # Chá»n precision dá»±a trÃªn VRAM
        if total_vram >= 16:
            torch_dtype = torch.float32
            print("  ğŸ¯ Using float32 precision (highest quality)")
        elif total_vram >= 12:
            torch_dtype = torch.float16
            print("  ğŸ¯ Using float16 precision (balanced)")
        else:
            torch_dtype = torch.float16
            print("  âš ï¸ Using float16 precision (limited VRAM)")
            
        device_map = "auto"
    else:
        print("  âš ï¸ WARNING: No CUDA device found. Model will run on CPU.")
        torch_dtype = torch.float32
        device_map = "cpu"
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=MODEL_CACHE_DIR,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
            low_cpu_mem_usage=True if torch.cuda.is_available() else False
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            cache_dir=MODEL_CACHE_DIR,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("  âœ… Original model loaded successfully!")
        return model, tokenizer
        
    except Exception as e:
        print(f"  âŒ Failed to load original model: {e}")
        return None, None


# === ORIGINAL MODEL BENCHMARK SUITE ===

class OriginalModelBenchmarkSuite:
    def __init__(self, model, tokenizer, model_hash: str):
        self.model = model
        self.tokenizer = tokenizer
        self.model_hash = model_hash
        self.device = next(model.parameters()).device
        self.model_precision = str(next(model.parameters()).dtype)
        
    def get_memory_usage(self) -> Dict[str, float]:
        """Láº¥y thÃ´ng tin sá»­ dá»¥ng bá»™ nhá»› (RAM & VRAM)."""
        process = psutil.Process()
        ram_gb = process.memory_info().rss / 1024**3
        
        gpu_allocated_gb = 0
        gpu_reserved_gb = 0
        if torch.cuda.is_available():
            gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
            gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
        
        return {
            'ram_gb': ram_gb, 
            'gpu_allocated_gb': gpu_allocated_gb,
            'gpu_reserved_gb': gpu_reserved_gb
        }
    
    def generate_response(self, prompt: str, task_name: str = "") -> Dict:
        """Sinh response vÃ  Ä‘o lÆ°á»ng hiá»‡u nÄƒng chi tiáº¿t."""
        print(f"    ğŸ”„ Generating response for: {task_name}")
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        memory_before = self.get_memory_usage()
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
              **inputs,
              max_new_tokens=512,
              do_sample=True,
              temperature=0.6,
              top_p=0.9,
              repetition_penalty=1.1,
              pad_token_id=self.tokenizer.pad_token_id,
              eos_token_id=self.tokenizer.eos_token_id,
              use_cache=True 
            )
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        memory_after = self.get_memory_usage()
        
        input_length = inputs['input_ids'].shape[1]
        response_tokens = outputs[0][input_length:]
        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
        
        generation_time = end_time - start_time
        tokens_generated = len(response_tokens)
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            'prompt': prompt,
            'response': response,
            'generation_time': round(generation_time, 2),
            'tokens_generated': tokens_generated,
            'tokens_per_second': round(tokens_per_second, 2),
            'vram_allocated_gb': round(memory_after['gpu_allocated_gb'], 2),
            'vram_reserved_gb': round(memory_after['gpu_reserved_gb'], 2)
        }

    def run_mmlu_test(self) -> Dict:
        """Cháº¡y MMLU (Massive Multitask Language Understanding) test."""
        print("  ğŸ§  Running MMLU Test...")
        
        correct_answers = 0
        total_questions = 0
        subject_results = {}
        
        for subject, questions in MMLU_QUESTIONS.items():
            print(f"    ğŸ“š Testing subject: {subject}")
            subject_correct = 0
            
            for i, q in enumerate(questions):
                # Format cÃ¢u há»i theo chuáº©n MMLU
                choices_text = "\n".join([f"{chr(65+j)}. {choice}" for j, choice in enumerate(q['choices'])])
                prompt = f"Question: {q['question']}\n{choices_text}\nAnswer:"
                
                # Generate response
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=5,  # Chá»‰ cáº§n 1 kÃ½ tá»± cho Ä‘Ã¡p Ã¡n
                        do_sample=False,   # Greedy decoding cho tÃ­nh nháº¥t quÃ¡n
                        temperature=0.1,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
                
                # Kiá»ƒm tra Ä‘Ã¡p Ã¡n
                predicted_letter = response[0].upper() if response and response[0].upper() in 'ABCD' else 'X'
                correct_letter = chr(65 + q['answer'])
                
                if predicted_letter == correct_letter:
                    subject_correct += 1
                    correct_answers += 1
                
                total_questions += 1
                
                print(f"      Q{i+1}: Predicted={predicted_letter}, Correct={correct_letter}, {'âœ“' if predicted_letter == correct_letter else 'âœ—'}")
            
            subject_accuracy = subject_correct / len(questions) * 100
            subject_results[subject] = {
                'correct': subject_correct,
                'total': len(questions),
                'accuracy': round(subject_accuracy, 2)
            }
            
            print(f"    ğŸ“Š {subject} accuracy: {subject_accuracy:.2f}%")
        
        overall_accuracy = correct_answers / total_questions * 100 if total_questions > 0 else 0
        
        return {
            'task': 'MMLU Test',
            'overall_accuracy': round(overall_accuracy, 2),
            'total_correct': correct_answers,
            'total_questions': total_questions,
            'subject_results': subject_results
        }

    def calculate_perplexity(self, text: str) -> float:
        """TÃ­nh perplexity cho má»™t Ä‘oáº¡n text."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss.item()
            perplexity = torch.exp(torch.tensor(loss)).item()
        
        return perplexity

    def run_perplexity_test(self) -> Dict:
        """Cháº¡y Perplexity test trÃªn nhiá»u Ä‘oáº¡n text khÃ¡c nhau."""
        print("  ğŸ“Š Running Perplexity Test...")
        
        perplexities = []
        passage_results = []
        
        for i, passage in enumerate(PERPLEXITY_PASSAGES):
            print(f"    ğŸ“ Testing passage {i+1}/{len(PERPLEXITY_PASSAGES)}")
            
            start_time = time.time()
            perplexity = self.calculate_perplexity(passage)
            calc_time = time.time() - start_time
            
            perplexities.append(perplexity)
            passage_results.append({
                'passage_id': i + 1,
                'perplexity': round(perplexity, 4),
                'calculation_time': round(calc_time, 3),
                'text_preview': passage[:100] + "..." if len(passage) > 100 else passage
            })
            
            print(f"      Perplexity: {perplexity:.4f} (calculated in {calc_time:.3f}s)")
        
        avg_perplexity = np.mean(perplexities)
        std_perplexity = np.std(perplexities)
        
        return {
            'task': 'Perplexity Test',
            'average_perplexity': round(avg_perplexity, 4),
            'std_perplexity': round(std_perplexity, 4),
            'min_perplexity': round(min(perplexities), 4),
            'max_perplexity': round(max(perplexities), 4),
            'passage_results': passage_results
        }
        
    def _run_single_benchmark(self, task_name: str, prompts: List[str]) -> Dict:
        print(f"  ğŸ“Š Benchmarking: {task_name}")
        results = []
        for i, p in enumerate(prompts):
            results.append(self.generate_response(p, f"{task_name} #{i+1}"))
            time.sleep(1) # Nghá»‰ giá»¯a cÃ¡c láº§n cháº¡y Ä‘á»ƒ á»•n Ä‘á»‹nh
        
        avg_time = np.mean([r['generation_time'] for r in results])
        avg_tps = np.mean([r['tokens_per_second'] for r in results])
        
        return {
            'task': task_name,
            'avg_generation_time': round(avg_time, 2),
            'avg_tokens_per_second': round(avg_tps, 2),
            'tests': results
        }

    def run_full_benchmark(self) -> Dict:
        """Cháº¡y toÃ n bá»™ bá»™ benchmark trÃªn model gá»‘c."""
        print("\nğŸš€ Starting Original Model Comprehensive Benchmark (NO QUANTIZATION)")
        print("=" * 80)
        print(f"ğŸ¯ Model Precision: {self.model_precision}")
        print(f"ğŸ’¾ Device: {self.device}")
        
        # === MMLU vÃ  Perplexity Tests ===
        print("\nğŸ§ª Running Academic & Technical Evaluations...")
        mmlu_results = self.run_mmlu_test()
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        perplexity_results = self.run_perplexity_test()
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # === Traditional Generation Tasks ===
        print("\nğŸ“ Running Traditional Generation Tasks...")
        tasks = {
            "Code Generation": [
                "Viáº¿t má»™t script Python sá»­ dá»¥ng thÆ° viá»‡n Pandas Ä‘á»ƒ Ä‘á»c file CSV cÃ³ tÃªn `sales_data.csv` vá»›i cÃ¡c cá»™t 'Date', 'Product', 'Revenue'. Script cáº§n tÃ­nh tá»•ng doanh thu theo tá»«ng sáº£n pháº©m vÃ  xuáº¥t káº¿t quáº£ ra má»™t file CSV má»›i cÃ³ tÃªn `revenue_by_product.csv`.",
                "Táº¡o má»™t component React functional báº±ng TypeScript tÃªn lÃ  `UserProfile`. Component nÃ y nháº­n vÃ o props lÃ  `name` (string), `age` (number), vÃ  `avatarUrl` (string), sau Ä‘Ã³ hiá»ƒn thá»‹ thÃ´ng tin nÃ y má»™t cÃ¡ch cÃ³ cáº¥u trÃºc."
            ],
            "Math Problem Solving": [
                "Má»™t bá»ƒ nÆ°á»›c cÃ³ hai vÃ²i. VÃ²i thá»© nháº¥t cháº£y má»™t mÃ¬nh thÃ¬ Ä‘áº§y bá»ƒ trong 4 giá». VÃ²i thá»© hai cháº£y má»™t mÃ¬nh thÃ¬ Ä‘áº§y bá»ƒ trong 6 giá». Náº¿u má»Ÿ cáº£ hai vÃ²i cÃ¹ng má»™t lÃºc khi bá»ƒ cáº¡n, há»i sau bao lÃ¢u thÃ¬ bá»ƒ sáº½ Ä‘áº§y? TrÃ¬nh bÃ y cÃ¡c bÆ°á»›c giáº£i chi tiáº¿t.",
                "Má»™t ngÆ°á»i gá»­i tiáº¿t kiá»‡m 500 triá»‡u Ä‘á»“ng vá»›i lÃ£i suáº¥t kÃ©p 6.5% má»—i nÄƒm. Há»i sau 5 nÄƒm, ngÆ°á»i Ä‘Ã³ sáº½ nháº­n Ä‘Æ°á»£c cáº£ vá»‘n láº«n lÃ£i lÃ  bao nhiÃªu tiá»n? YÃªu cáº§u trÃ¬nh bÃ y cÃ´ng thá»©c vÃ  cÃ¡c bÆ°á»›c tÃ­nh toÃ¡n."
            ],
            "Text Summarization": [
                "HÃ£y tÃ³m táº¯t Ä‘oáº¡n vÄƒn sau thÃ nh 3 Ã½ chÃ­nh: 'CÃ¡c cÃ´ng nghá»‡ thu giá»¯ carbon (Carbon Capture Technologies) Ä‘ang ná»•i lÃªn nhÆ° má»™t giáº£i phÃ¡p tiá»m nÄƒng trong cuá»™c chiáº¿n chá»‘ng biáº¿n Ä‘á»•i khÃ­ háº­u. CÃ¡c phÆ°Æ¡ng phÃ¡p chÃ­nh bao gá»“m thu giá»¯ sau Ä‘á»‘t chÃ¡y, thu giá»¯ trÆ°á»›c Ä‘á»‘t chÃ¡y vÃ  thu giá»¯ tá»« khÃ´ng khÃ­ trá»±c tiáº¿p (DAC). Máº·c dÃ¹ cÃ³ tiá»m nÄƒng lá»›n, cÃ´ng nghá»‡ nÃ y váº«n Ä‘á»‘i máº·t vá»›i nhá»¯ng thÃ¡ch thá»©c vá» chi phÃ­ váº­n hÃ nh cao, hiá»‡u quáº£ nÄƒng lÆ°á»£ng vÃ  váº¥n Ä‘á» lÆ°u trá»¯ carbon an toÃ n trong dÃ i háº¡n. CÃ¡c chÃ­nh phá»§ vÃ  táº­p Ä‘oÃ n lá»›n Ä‘ang Ä‘áº§u tÆ° hÃ ng tá»· USD vÃ o R&D Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ vÃ  giáº£m giÃ¡ thÃ nh, hy vá»ng biáº¿n nÃ³ thÃ nh má»™t cÃ´ng cá»¥ chá»§ chá»‘t vÃ o nÄƒm 2050.'",
                "TÃ³m táº¯t ngáº¯n gá»n nhá»¯ng sá»± kiá»‡n chÃ­nh vÃ  Ã½ nghÄ©a lá»‹ch sá»­ cá»§a cuá»™c CÃ¡ch máº¡ng CÃ´ng nghiá»‡p láº§n thá»© nháº¥t, táº­p trung vÃ o cÃ¡c phÃ¡t minh quan trá»ng vÃ  tÃ¡c Ä‘á»™ng cá»§a nÃ³ Ä‘áº¿n xÃ£ há»™i."
            ],
            "Creative Writing": [
                "Viáº¿t Ä‘oáº¡n má»Ÿ Ä‘áº§u cho má»™t cÃ¢u chuyá»‡n ngáº¯n thuá»™c thá»ƒ loáº¡i khoa há»c viá»…n tÆ°á»Ÿng, trong Ä‘Ã³ nhÃ¢n váº­t chÃ­nh lÃ  má»™t nhÃ  thá»±c váº­t há»c sá»‘ng trÃªn Sao Há»a, ngÆ°á»i vá»«a phÃ¡t hiá»‡n ra má»™t loÃ i cÃ¢y cÃ³ kháº£ nÄƒng giao tiáº¿p báº±ng Ã¡nh sÃ¡ng.",
                "Viáº¿t má»™t bÃ i vÄƒn ngáº¯n (khoáº£ng 150 tá»«) miÃªu táº£ cáº£nh má»™t phiÃªn chá»£ ná»•i trÃªn sÃ´ng Cá»­u Long vÃ o buá»•i sÃ¡ng sá»›m, táº­p trung vÃ o Ã¢m thanh, mÃ u sáº¯c vÃ  mÃ¹i vá»‹ Ä‘áº·c trÆ°ng."
            ],
            "Question Answering": [
                "Giáº£i thÃ­ch sá»± khÃ¡c biá»‡t cÆ¡ báº£n giá»¯a nÄƒng lÆ°á»£ng háº¡t nhÃ¢n phÃ¢n háº¡ch (nuclear fission) vÃ  tá»•ng há»£p háº¡t nhÃ¢n (nuclear fusion). Loáº¡i nÃ o hiá»‡n Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nhÃ  mÃ¡y Ä‘iá»‡n?",
                "Con Ä‘Æ°á»ng tÆ¡ lá»¥a lÃ  gÃ¬ vÃ  nÃ³ cÃ³ vai trÃ² quan trá»ng nhÆ° tháº¿ nÃ o Ä‘á»‘i vá»›i sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c ná»n vÄƒn minh cá»• Ä‘áº¡i?"
            ],
            "Language Translation & Nuance": [
                "Dá»‹ch Ä‘oáº¡n vÄƒn sau sang tiáº¿ng Viá»‡t, chÃº Ã½ giá»¯ vÄƒn phong chuyÃªn nghiá»‡p: 'Our team is conducting a comprehensive due diligence process to assess the viability of the potential acquisition. Key performance indicators (KPIs) and financial statements are under rigorous scrutiny.'",
                "Giáº£i thÃ­ch Ã½ nghÄ©a vÃ  tÃ¬m cÃ¢u thÃ nh ngá»¯ tiáº¿ng Anh cÃ³ nghÄ©a tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¢u 'NÆ°á»›c Ä‘áº¿n chÃ¢n má»›i nháº£y'."
            ],
            "Reasoning & Logic": [
                "CÃ³ ba chiáº¿c há»™p. Má»™t há»™p chá»©a toÃ n bi Ä‘á», má»™t há»™p chá»©a toÃ n bi xanh, vÃ  má»™t há»™p chá»©a láº«n lá»™n cáº£ bi Ä‘á» vÃ  bi xanh. Cáº£ ba há»™p Ä‘á»u bá»‹ dÃ¡n nhÃ£n sai. Báº¡n chá»‰ Ä‘Æ°á»£c phÃ©p láº¥y ra má»™t viÃªn bi tá»« má»™t há»™p duy nháº¥t (khÃ´ng Ä‘Æ°á»£c nhÃ¬n vÃ o bÃªn trong). LÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c ná»™i dung cá»§a cáº£ ba há»™p? HÃ£y giáº£i thÃ­ch quÃ¡ trÃ¬nh suy luáº­n cá»§a báº¡n.",
                "Má»™t nghiÃªn cá»©u cho tháº¥y nhá»¯ng thÃ nh phá»‘ cÃ³ nhiá»u cá»­a hÃ ng kem nháº¥t cÅ©ng cÃ³ tá»· lá»‡ tá»™i pháº¡m cao nháº¥t. CÃ³ pháº£i Äƒn kem gÃ¢y ra tá»™i pháº¡m khÃ´ng? HÃ£y giáº£i thÃ­ch vá» má»‘i tÆ°Æ¡ng quan vÃ  quan há»‡ nhÃ¢n quáº£ (correlation vs. causation) trong trÆ°á»ng há»£p nÃ y."
            ],
            "Technical Explanation": [
                "HÃ£y giáº£i thÃ­ch cho má»™t ngÆ°á»i khÃ´ng rÃ nh vá» cÃ´ng nghá»‡ vá» khÃ¡i niá»‡m 'Äiá»‡n toÃ¡n Ä‘Ã¡m mÃ¢y' (Cloud Computing) lÃ  gÃ¬. Sá»­ dá»¥ng vÃ­ dá»¥ vá» Google Drive hoáº·c iCloud Ä‘á»ƒ minh há»a.",
                "Giáº£i thÃ­ch má»™t cÃ¡ch Ä‘Æ¡n giáº£n quÃ¡ trÃ¬nh quang há»£p á»Ÿ thá»±c váº­t diá»…n ra nhÆ° tháº¿ nÃ o vÃ  táº¡i sao nÃ³ láº¡i quan trá»ng Ä‘á»‘i vá»›i sá»± sá»‘ng trÃªn TrÃ¡i Äáº¥t."
            ]
        }

        benchmark_results = []
        for name, prompts in tasks.items():
            benchmark_results.append(self._run_single_benchmark(name, prompts))
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None

        # === Tá»•ng há»£p káº¿t quáº£ ===
        overall_tps = [r['avg_tokens_per_second'] for r in benchmark_results]
        memory_info = self.get_memory_usage()
        
        overall_stats = {
            'avg_token_speed': round(np.mean(overall_tps), 2) if overall_tps else 0,
            'mmlu_accuracy': mmlu_results['overall_accuracy'],
            'avg_perplexity': perplexity_results['average_perplexity'],
            'model_precision': self.model_precision,
            'peak_vram_allocated_gb': round(memory_info['gpu_allocated_gb'], 2),
            'peak_vram_reserved_gb': round(memory_info['gpu_reserved_gb'], 2)
        }
        
        return {
            'model_type': 'original_no_quantization',
            'model_hash': self.model_hash,
            'timestamp': datetime.now().isoformat(),
            'overall_stats': overall_stats,
            'mmlu_results': mmlu_results,
            'perplexity_results': perplexity_results,
            'benchmark_results': benchmark_results
        }

    def save_and_print_summary(self, results: Dict):
        """LÆ°u vÃ  in káº¿t quáº£ benchmark."""
        stats = results['overall_stats']
        print("\n" + "="*80 + "\nğŸ“Š ORIGINAL MODEL BENCHMARK RESULTS SUMMARY\n" + "="*80)
        print(f"ğŸ¯ Model Type: Original (No Quantization)")
        print(f"ğŸ”‘ Model Hash: {results['model_hash']}")
        print(f"ğŸ­ Model Precision: {stats['model_precision']}")
        print(f"ğŸ“ˆ Average Token Speed (Overall): {stats['avg_token_speed']:.2f} tokens/sec")
        print(f"ğŸ§  MMLU Accuracy: {stats['mmlu_accuracy']:.2f}%")
        print(f"ğŸ“Š Average Perplexity: {stats['avg_perplexity']:.4f}")
        print(f"ğŸ’¾ Peak VRAM Usage: {stats['peak_vram_allocated_gb']:.2f} GB (Reserved: {stats['peak_vram_reserved_gb']:.2f} GB)")
        
        print("\nğŸ§ª Academic Test Results:")
        print(f"  ğŸ“š MMLU: {results['mmlu_results']['total_correct']}/{results['mmlu_results']['total_questions']} correct ({stats['mmlu_accuracy']:.2f}%)")
        
        mmlu_by_subject = results['mmlu_results']['subject_results']
        for subject, data in mmlu_by_subject.items():
            print(f"    - {subject:<20}: {data['accuracy']:>6.2f}%")
        
        print(f"  ğŸ“Š Perplexity Range: {results['perplexity_results']['min_perplexity']:.4f} - {results['perplexity_results']['max_perplexity']:.4f}")
        
        print("\nğŸ“‹ Generation Task Performance:")
        for result in results['benchmark_results']:
            vram = result['tests'][0]['vram_allocated_gb'] if result['tests'] else 'N/A'
            print(f"  - {result['task']:<25}: {result['avg_tokens_per_second']:.2f} tok/s @ {vram} GB VRAM")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"original_model_benchmark_{self.model_hash}_{timestamp}.json"
        filepath = Path("benchmark_results") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Full detailed results saved to: {filepath}")

# === HÃ€M MAIN Äá»‚ CHáº Y ORIGINAL MODEL BENCHMARK ===
def main():
    print("ğŸ” Original Model Benchmark System (No EWQ Quantization)")
    print("=" * 80)
    
    try:
        model_hash = get_model_hash(MODEL_ID)
        print(f"ğŸ”‘ Model ID: {MODEL_ID}")
        print(f"ğŸ”‘ Model Hash: {model_hash}")
        
        model, tokenizer = load_original_model(MODEL_ID)
        
        if model is None or tokenizer is None:
            print("âŒ Failed to load original model!")
            return
        
        print("âœ… Original model loaded and ready for benchmarking!")
        
        # Khá»Ÿi táº¡o benchmark suite
        benchmark_suite = OriginalModelBenchmarkSuite(model, tokenizer, model_hash)
        
        # Cháº¡y toÃ n bá»™ benchmark
        start_time = time.time()
        results = benchmark_suite.run_full_benchmark()
        total_time = time.time() - start_time
        
        # ThÃªm thÃ´ng tin thá»i gian tá»•ng thá»ƒ
        results['total_benchmark_time'] = round(total_time, 2)
        
        # LÆ°u vÃ  in káº¿t quáº£
        benchmark_suite.save_and_print_summary(results)
        
        print(f"\nâ±ï¸ Total benchmark time: {total_time:.2f} seconds")
        print("\nğŸ‰ Original model benchmark completed successfully!")
        
        # Dá»n dáº¹p bá»™ nhá»›
        del model
        del tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Benchmark interrupted by user.")
        
    except Exception as e:
        print(f"\nâŒ Error during benchmark: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Dá»n dáº¹p cuá»‘i cÃ¹ng
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("\nğŸ§¹ Memory cleanup completed.")


if __name__ == "__main__":
    main()