#!/usr/bin/env python3
"""
EWQ Plan Generator (Tensor-Level, CPU-based) - Fixed Version

This script generates a quantization plan using tensor-level entropy analysis
for the most fine-grained optimization, including Linear, LayerNorm, and other tensors.
"""
import torch
import torch.nn as nn
import numpy as np
import gc
import os
import json
import re
from pathlib import Path
from transformers import AutoModelForCausalLM
from typing import Dict, Tuple, List, Any
from benchmark.utils import get_model_hash, get_plan_path

# === C·∫•u h√¨nh (C√≥ th·ªÉ thay ƒë·ªïi) ===
MODEL_ID = "Qwen/Qwen3-8B"
MODEL_CACHE_DIR = "./models"
ENTROPY_THRESHOLD_FACTOR = 0.55  # ƒêi·ªÅu ch·ªânh tinh t·∫ø

# TENSOR type importance weights for decision making (Chi ti·∫øt h∆°n)
TENSOR_IMPORTANCE = {
    'output.weight': 1.9,           # TƒÉng l√™n ƒë·ªÉ ƒë·∫£m b·∫£o f16
    'token_embd.weight': 1.9,       # TƒÉng l√™n ƒë·ªÉ ƒë·∫£m b·∫£o f16
    'attn_q.weight': 1.45,          # TƒÉng l√™n m·ªôt ch√∫t
    'attn_k.weight': 1.35,          # TƒÉng l√™n m·ªôt ch√∫t
    'attn_v.weight': 1.35,          # TƒÉng l√™n m·ªôt ch√∫t
    'attn_output.weight': 1.25,     # TƒÉng l√™n m·ªôt ch√∫t
    'ffn_gate.weight': 1.05,        # TƒÉng l√™n m·ªôt ch√∫t
    'ffn_up.weight': 1.05,          # TƒÉng l√™n m·ªôt ch√∫t
    'ffn_down.weight': 1.15,        # TƒÉng l√™n m·ªôt ch√∫t
    'attn_norm.weight': 1.55,       # TƒÉng l√™n m·ªôt ch√∫t
    'ffn_norm.weight': 1.55,        # TƒÉng l√™n m·ªôt ch√∫t
    'tok_embeddings.weight': 1.9,   # TƒÉng l√™n ƒë·ªÉ ƒë·∫£m b·∫£o f16
    'output_norm.weight': 1.8,      # TƒÉng l√™n ƒë·ªÉ ƒë·∫£m b·∫£o f16
    # M·∫∑c ƒë·ªãnh
    '.weight': 1.0,
    '.bias': 1.25,                  # TƒÉng l√™n m·ªôt ch√∫t
}

def convert_to_json_serializable(obj):
    if isinstance(obj, np.integer): return int(obj)
    if isinstance(obj, np.floating): return float(obj)
    if isinstance(obj, np.ndarray): return obj.tolist()
    if isinstance(obj, dict): return {k: convert_to_json_serializable(v) for k, v in obj.items()}
    if isinstance(obj, list): return [convert_to_json_serializable(i) for i in obj]
    return obj

def convert_hf_name_to_gguf(hf_name: str) -> str:
    """Converts a Hugging Face tensor name to a GGUF compatible name."""
    # X√≥a prefix 'model.'
    name = hf_name.replace("model.", "")
    
    # √Ånh x·∫° c√°c layer ƒë·∫∑c bi·ªát
    if name == "embed_tokens.weight":
        return "token_embd.weight"
    if name == "lm_head.weight":
        return "output.weight"
    if name == "norm.weight":
        return "output_norm.weight"

    # √Ånh x·∫° c√°c block
    # v√≠ d·ª•: layers.10.self_attn.q_proj.weight -> blk.10.attn_q.weight
    m = re.match(r"layers\.(\d+)\.(self_attn|mlp)\.(.+)\.(weight|bias)", name)
    if not m:
        # X·ª≠ l√Ω Layernorm trong block
        m = re.match(r"layers\.(\d+)\.(input_layernorm|post_attention_layernorm)\.(weight|bias)", name)
        if not m:
            return name # Tr·∫£ v·ªÅ t√™n g·ªëc n·∫øu kh√¥ng kh·ªõp
        
        block_idx, norm_type, tensor_type = m.groups()
        norm_map = {
            'input_layernorm': 'attn_norm',
            'post_attention_layernorm': 'ffn_norm'
        }
        return f"blk.{block_idx}.{norm_map[norm_type]}.{tensor_type}"

    block_idx, block_type, layer_name, tensor_type = m.groups()

    if block_type == "self_attn":
        layer_map = {
            'q_proj': 'attn_q',
            'k_proj': 'attn_k',
            'v_proj': 'attn_v',
            'o_proj': 'attn_output'
        }
        gguf_layer_name = layer_map.get(layer_name, layer_name)
    elif block_type == "mlp":
        layer_map = {
            'gate_proj': 'ffn_gate',
            'up_proj': 'ffn_up',
            'down_proj': 'ffn_down'
        }
        gguf_layer_name = layer_map.get(layer_name, layer_name)
    else:
        gguf_layer_name = layer_name

    return f"blk.{block_idx}.{gguf_layer_name}.{tensor_type}"

def get_tensor_importance(gguf_name: str) -> float:
    """Determines the importance weight of a tensor based on its GGUF name."""
    # ∆Øu ti√™n kh·ªõp ch√≠nh x√°c nh·∫•t
    for pattern, weight in sorted(TENSOR_IMPORTANCE.items(), key=lambda x: len(x[0]), reverse=True):
        if pattern in gguf_name:
            return weight
    return 1.0  # Default importance

def calculate_tensor_entropy(tensor_weights: torch.Tensor) -> Tuple[float, Dict[str, Any]]:
    """Calculates entropy and statistical metrics for a single tensor."""
    weights_flat = tensor_weights.detach().cpu().float().flatten().numpy()
    
    if weights_flat.size == 0:
        return 0.0, {}

    hist, _ = np.histogram(weights_flat, bins=256, density=True)
    hist = hist[hist > 0]
    shannon_entropy = -np.sum(hist * np.log2(hist))
    
    metrics = {
        'shannon_entropy': shannon_entropy,
        'std': np.std(weights_flat),
        'mean_abs': np.mean(np.abs(weights_flat)),
        'sparsity': np.sum(np.abs(weights_flat) < 1e-6) / len(weights_flat),
        'tensor_size': len(weights_flat)
    }
    
    del weights_flat
    return float(shannon_entropy), metrics

def analyze_tensor_by_tensor(model: nn.Module) -> List[Dict]:
    """Analyzes entropy for each individual tensor in the model."""
    print("üî¨ Analyzing entropy tensor by tensor...")
    tensor_results = []
    
    # Iterate through all model parameters
    total_tensors = len(list(model.named_parameters()))
    print(f"  Total tensors to analyze: {total_tensors}")

    for i, (hf_name, tensor) in enumerate(model.named_parameters()):
        gguf_name = convert_hf_name_to_gguf(hf_name)
        if "lora" in gguf_name: # B·ªè qua LoRA adapters n·∫øu c√≥
            continue

        entropy, metrics = calculate_tensor_entropy(tensor)
        importance = get_tensor_importance(gguf_name)
        metrics['tensor_importance'] = importance
        
        tensor_result = {
            'hf_name': hf_name,
            'gguf_name': gguf_name,
            'entropy': entropy,
            'metrics': metrics
        }
        tensor_results.append(tensor_result)
        
        if (i+1) % 50 == 0 or (i+1) == total_tensors:
            print(f"  ‚úÖ Processed {i+1}/{total_tensors}: {gguf_name} | entropy={entropy:.4f} | importance={importance:.2f}")

    print(f"\nüìà Analyzed {len(tensor_results)} tensors.")
    return tensor_results

def create_tensorwise_quantization_plan(tensor_results: List[Dict], entropy_factor: float) -> Tuple[Dict[str, str], Dict]:
    """Creates a detailed quantization plan for individual tensors."""
    print("üéØ Creating tensor-wise quantization plan...")
    
    # Ph√¢n t√≠ch ph√¢n ph·ªëi entropy (c√≥ th·ªÉ th√™m chi ti·∫øt n·∫øu c·∫ßn)
    entropies = np.array([r['entropy'] for r in tensor_results if r['entropy'] > 0])
    importances = np.array([r['metrics']['tensor_importance'] for r in tensor_results if r['entropy'] > 0])
    
    mean_entropy = np.mean(entropies)
    std_entropy = np.std(entropies)
    
    # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ƒë·ªÉ c√≥ ph√¢n ph·ªëi c√¢n b·∫±ng h∆°n
    high_precision_threshold = mean_entropy + entropy_factor * std_entropy
    medium_precision_threshold = mean_entropy - entropy_factor * std_entropy  # Thay ƒë·ªïi c√¥ng th·ª©c
    
    print(f"\nüéØ Calculated Thresholds:")
    print(f"  - High Precision (f16): entropy >= {high_precision_threshold:.4f} (adjusted by importance)")
    print(f"  - Medium Precision (Q8_0): entropy >= {medium_precision_threshold:.4f} (adjusted by importance)")
    print(f"  - Low Precision (Q4_K): entropy < {medium_precision_threshold:.4f}")
    
    plan = {}
    counts = {"f16": 0, "Q8_0": 0, "Q4_K": 0, "COPY": 0} # COPY cho c√°c tensor nh·ªè
    
    # --- QUY T·∫ÆC C·ª®NG ---
    # Lu√¥n gi·ªØ c√°c tensor ƒë·∫∑c bi·ªát ·ªü ƒë·ªô ch√≠nh x√°c cao
    critical_tensors = ["output.weight", "token_embd.weight", "output_norm.weight"]
    for tensor_name in critical_tensors:
        plan[tensor_name] = "f16"
    
    for r in tensor_results:
        gguf_name = r['gguf_name']
        if gguf_name in plan: # ƒê√£ c√≥ quy t·∫Øc c·ª©ng
            counts[plan[gguf_name]] += 1
            continue

        entropy = r['entropy']
        importance = r['metrics']['tensor_importance']
        tensor_size = r['metrics']['tensor_size']
        
        # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng COPY - ch·ªâ √°p d·ª•ng cho tensor r·∫•t nh·ªè
        if tensor_size < 128:  # Gi·∫£m t·ª´ 512 xu·ªëng 128
            decision = "COPY"
        else:
            # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng d·ª±a tr√™n ƒë·ªô quan tr·ªçng c·ªßa tensor
            # Tensor c√†ng quan tr·ªçng, ng∆∞·ª°ng ƒë·ªÉ b·ªã quantize xu·ªëng th·∫•p c√†ng cao
            adjusted_high = high_precision_threshold / max(importance, 1.0)  # Tr√°nh chia cho 0
            adjusted_medium = medium_precision_threshold / max(importance, 1.0)
            
            # Th√™m logic ƒë·∫∑c bi·ªát cho c√°c tensor quan tr·ªçng
            # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng ƒë·ªÉ c√≥ ph√¢n ph·ªëi h·ª£p l√Ω: ~10% f16, ~35% Q8_0, ~55% Q4_K
            if importance >= 1.55 or entropy >= adjusted_high:  # Gi·∫£m xu·ªëng 1.55
                decision = "f16"
            elif importance >= 1.2 or entropy >= adjusted_medium:  # TƒÉng l√™n 1.2
                decision = "Q8_0"
            else:
                decision = "Q4_K"
        
        plan[gguf_name] = decision
        counts[decision] += 1
        
    print("\nüéØ Final Tensor-wise Quantization Plan:")
    print(f"  - f16 (High Precision): {counts['f16']} tensors")
    print(f"  - Q8_0 (Medium):        {counts['Q8_0']} tensors")
    print(f"  - Q4_K (Low):           {counts['Q4_K']} tensors")
    print(f"  - COPY (Untouched):     {counts['COPY']} tensors")
    print(f"  - Total tensors:        {sum(counts.values())}")
    
    # Th√™m ph√¢n t√≠ch ph·∫ßn trƒÉm
    total = sum(counts.values())
    print(f"\nüìä Distribution Percentages:")
    for quant_type, count in counts.items():
        percentage = (count / total) * 100
        print(f"  - {quant_type}: {percentage:.1f}%")
    
    analysis = {
        'plan_statistics': counts,
        'total_tensors': len(tensor_results)
    }
    
    return plan, analysis

def generate_tensorwise_llama_quantize_command(plan: Dict[str, str], model_id: str) -> str:
    """Generates the llama-quantize command from a tensor-wise plan - FIXED VERSION."""
    tensor_types_str = []
    
    # Ki·ªÉm tra v√† validate plan tr∆∞·ªõc khi t·∫°o command
    valid_quant_types = {"f16", "Q8_0", "Q4_K", "Q6_K", "Q5_0", "Q5_1", "COPY"}
    
    # Th√™m c√°c tensor v√†o command v·ªõi validation
    for tensor_name, quant_type in sorted(plan.items()):
        if quant_type == "COPY": # Kh√¥ng c·∫ßn ch·ªâ ƒë·ªãnh cho COPY
            continue
            
        # Validate quant_type
        if quant_type not in valid_quant_types:
            print(f"‚ö†Ô∏è  Warning: Invalid quantization type '{quant_type}' for tensor '{tensor_name}', skipping...")
            continue
            
        # Validate tensor_name (kh√¥ng ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y l·ªói)
        if not tensor_name or '"' in tensor_name or '\n' in tensor_name:
            print(f"‚ö†Ô∏è  Warning: Invalid tensor name '{tensor_name}', skipping...")
            continue
            
        tensor_types_str.append(f'--tensor-type "{tensor_name}={quant_type}"')
    
    # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ tensor n√†o h·ª£p l·ªá
    if not tensor_types_str:
        print("‚ö†Ô∏è  Warning: No valid tensor specifications found!")
        return ""
    
    formatted_tensor_types = " \\\n  ".join(tensor_types_str)
    model_name = model_id.split('/')[-1]

    # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
    command = f"""#!/bin/bash

# Auto-generated tensor-wise quantization command based on EWQ plan.
# Fixed version with proper error handling and validation

# Ensure output directory exists
mkdir -p ./models

# Check if input file exists
if [ ! -f "./models/{model_name}-F16.gguf" ]; then
    echo "‚ùå Error: Input file ./models/{model_name}-F16.gguf not found!"
    exit 1
fi

# Check if imatrix file exists
if [ ! -f "./namtb/{model_name}-imatrix.dat" ]; then
    echo "‚ö†Ô∏è  Warning: imatrix file ./namtb/{model_name}-imatrix.dat not found!"
    echo "Continuing without imatrix..."
    IMATRIX_ARG=""
else
    IMATRIX_ARG="--imatrix ./namtb/{model_name}-imatrix.dat"
fi

# Run quantization with error handling
echo "üöÄ Starting quantization..."
./build/bin/llama-quantize \\
  --allow-requantize \\
  $IMATRIX_ARG \\
  {formatted_tensor_types} \\
  ./models/{model_name}-F16.gguf \\
  ./models/{model_name}-gguf-ewq-tensorwise.gguf \\
  Q4_K \\
  8

# Check result
if [ $? -eq 0 ]; then
    echo "‚úÖ Quantization completed successfully!"
    echo "üìÅ Output: ./models/{model_name}-gguf-ewq-tensorwise.gguf"
else
    echo "‚ùå Quantization failed!"
    exit 1
fi"""
    
    return command

# Ph·∫ßn main v·ªõi error handling t·ªët h∆°n
def main():
    print("üöÄ EWQ Tensor-Level Plan Generator (CPU-based) - FIXED VERSION")
    print("=" * 70)
    
    try:
        model_config = {
            'base_model': MODEL_ID, 
            'entropy_factor': ENTROPY_THRESHOLD_FACTOR,
            'quant_method': 'ewq-tensorwise', # ƒê·ªïi t√™n
            'granularity': 'tensor' # ƒê·ªïi t√™n
        }
        model_hash = get_model_hash(MODEL_ID, model_config)
        plan_path = get_plan_path(model_hash)

        if plan_path.exists():
            print(f"‚úÖ Plan already exists: {plan_path}. Nothing to do.")
            return

        print(f"üîë Model ID: {MODEL_ID}\nüîë Config Hash: {model_hash}")
        print(f"üéØ Granularity: Tensor-level (Optimal)")
        
        print("\nüì• Loading base model to CPU...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            cache_dir=MODEL_CACHE_DIR,
            torch_dtype=torch.float16,
            device_map="cpu",
            trust_remote_code=True,
        )
        print("‚úÖ Base model loaded to CPU.")
        
        # Ph√¢n t√≠ch theo t·ª´ng tensor
        tensor_results = analyze_tensor_by_tensor(model)
        
        # T·∫°o k·∫ø ho·∫°ch quantize theo tensor
        plan, analysis = create_tensorwise_quantization_plan(tensor_results, ENTROPY_THRESHOLD_FACTOR)
        
        # L∆∞u k·∫ø ho·∫°ch
        save_data = {
            'plan': plan,
            'analysis_summary': {
                'total_tensors': analysis['total_tensors'],
                'plan_statistics': analysis['plan_statistics'],
                'granularity': 'tensor-level'
            }
        }

        print(f"\nüíæ Saving tensor-wise quantization plan to: {plan_path}")
        plan_path.parent.mkdir(parents=True, exist_ok=True)
        with open(plan_path, 'w') as f:
            json.dump(convert_to_json_serializable(save_data), f, indent=2)
        
        # T·∫°o command v·ªõi error handling
        command_str = generate_tensorwise_llama_quantize_command(plan, MODEL_ID)
        if not command_str:
            print("‚ùå Failed to generate quantization command!")
            return
            
        command_path = plan_path.with_name(f"quantize_command_tensorwise_{model_hash}.sh")
        
        print(f"üíæ Saving quantization command script to: {command_path}")
        with open(command_path, 'w') as f:
            f.write(command_str)
        
        os.chmod(command_path, 0o755)

        print("\nüéä Tensor-wise plan generation complete!")
        print(f"üìä Generated plan for {analysis['total_tensors']} individual tensors")
        print("You can now run the quantization command with better error handling.")

        del model
        gc.collect()
        
    except Exception as e:
        print(f"‚ùå Error during plan generation: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()