#!/usr/bin/env python3
"""
EWQ Model Benchmark System (v4 - VRAM Optimized with Full Logging)

Quy trÃ¬nh:
1. Táº£i model gá»‘c lÃªn CPU.
2. Táº£i káº¿ hoáº¡ch lÆ°á»£ng tá»­ hÃ³a Ä‘Ã£ cache.
3. Ãp dá»¥ng káº¿ hoáº¡ch lÃªn model trÃªn CPU.
4. Di chuyá»ƒn model Ä‘Ã£ lÆ°á»£ng tá»­ hÃ³a sang GPU.
5. Cháº¡y bá»™ benchmark toÃ n diá»‡n vá»›i Ä‘áº§y Ä‘á»§ logging.
"""
import torch
import torch.nn as nn
import time
import psutil
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Callable, Optional
from datetime import datetime
import gc
import os
import warnings
from transformers import AutoModelForCausalLM, AutoTokenizer
import bitsandbytes.nn as bnb

warnings.filterwarnings("ignore")

# === Cáº¥u hÃ¬nh (Pháº£i khá»›p vá»›i file plan generator) ===
MODEL_ID = "unsloth/Meta-Llama-3.1-8B-Instruct"
MODEL_CACHE_DIR = "./models"
QUANTIZED_MODEL_CACHE_DIR = "./quantized_models"
ENTROPY_THRESHOLD_FACTOR = 1.0

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# === LOGIC LÆ¯á»¢NG Tá»¬ HÃ“A - SAO CHÃ‰P Tá»ª main_cache_model.py Äá»‚ Äáº¢M Báº¢O NHáº¤T QUÃN ===

def get_model_hash(model_id: str, config: dict) -> str:
    config_str = f"{model_id}-{json.dumps(config, sort_keys=True)}"
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]

def get_plan_path(model_hash: str) -> Path:
    cache_dir = Path(QUANTIZED_MODEL_CACHE_DIR)
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir / f"quant_plan_{model_hash}.json"

def _find_and_replace(module: nn.Module, replacement_func: Callable, name_prefix=""):
    for name, child in module.named_children():
        full_name = f"{name_prefix}.{name}" if name_prefix else name
        if isinstance(child, nn.Linear):
            setattr(module, name, replacement_func(child, full_name))
        else:
            _find_and_replace(child, replacement_func, name_prefix=full_name)

def apply_quantization_balanced(model: nn.Module, plan: Dict[int, str]) -> nn.Module:
    print("  ğŸ”§ Applying balanced quantization plan on CPU...")
    for block_idx, quant_type in plan.items():
        if quant_type == "raw": continue
        block = model.model.layers[block_idx]
        block.to(torch.float16)
        def replacement_function(linear_module, module_name):
            if quant_type == "8-bit":
                q_layer = bnb.Linear8bitLt(linear_module.in_features, linear_module.out_features, bias=linear_module.bias is not None, has_fp16_weights=False)
            elif quant_type == "4-bit":
                q_layer = bnb.Linear4bit(linear_module.in_features, linear_module.out_features, bias=linear_module.bias is not None, compute_dtype=torch.float16, quant_type="nf4")
            else:
                return linear_module
            q_layer.weight.data.copy_(linear_module.weight.data)
            if linear_module.bias is not None:
                q_layer.bias.data.copy_(linear_module.bias.data)
            return q_layer
        _find_and_replace(block, replacement_function)
    print("  âœ… Quantization plan applied successfully on CPU model!")
    return model

def check_quantized_model_exists(model_hash: str) -> bool:
    return get_plan_path(model_hash).exists()

def load_quantized_model(model_id: str, model_hash: str) -> Tuple[Optional[nn.Module], Optional[AutoTokenizer]]:
    plan_path = get_plan_path(model_hash)
    print(f"  ğŸ“„ Loading quantization plan from: {plan_path}")
    with open(plan_path, 'r') as f:
        quant_plan = {int(k): v for k, v in json.load(f).items()}

    print("  ğŸ“¥ Loading base model to CPU...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id, cache_dir=MODEL_CACHE_DIR, torch_dtype=torch.float16,
        device_map="cpu", trust_remote_code=True)
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_id, cache_dir=MODEL_CACHE_DIR, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    
    quantized_model = apply_quantization_balanced(model, quant_plan)
    
    print("  ğŸš€ Deploying quantized model to GPU...")
    if torch.cuda.is_available():
        quantized_model.to("cuda")
    else:
        print("  âš ï¸ WARNING: No CUDA device found. Benchmark will run on CPU.")

    return quantized_model, tokenizer


# === BENCHMARK SUITE Vá»šI LOGGING Äáº¦Y Äá»¦ ===

class BenchmarkSuite:
    def __init__(self, model, tokenizer, model_hash: str):
        self.model = model
        self.tokenizer = tokenizer
        self.model_hash = model_hash
        self.device = next(model.parameters()).device
        
    def get_memory_usage(self) -> Dict[str, float]:
        """Láº¥y thÃ´ng tin sá»­ dá»¥ng bá»™ nhá»› (RAM & VRAM)."""
        process = psutil.Process()
        ram_gb = process.memory_info().rss / 1024**3
        
        gpu_allocated_gb = 0
        if torch.cuda.is_available():
            gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
        
        return {'ram_gb': ram_gb, 'gpu_allocated_gb': gpu_allocated_gb}
    
    def generate_response(self, prompt: str, task_name: str = "") -> Dict:
        """Sinh response vÃ  Ä‘o lÆ°á»ng hiá»‡u nÄƒng chi tiáº¿t."""
        print(f"    ğŸ”„ Generating response for: {task_name}")
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        torch.cuda.synchronize()
        memory_before = self.get_memory_usage()
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
              **inputs,
              max_new_tokens=512,
              do_sample=True,
              temperature=0.6,
              top_p=0.9,
              repetition_penalty=1.1,
              pad_token_id=self.tokenizer.pad_token_id,
              eos_token_id=self.tokenizer.eos_token_id,
              use_cache=True 
            )
        
        torch.cuda.synchronize()
        end_time = time.time()
        memory_after = self.get_memory_usage()
        
        input_length = inputs['input_ids'].shape[1]
        response_tokens = outputs[0][input_length:]
        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
        
        generation_time = end_time - start_time
        tokens_generated = len(response_tokens)
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            'prompt': prompt,
            'response': response,
            'generation_time': round(generation_time, 2),
            'tokens_generated': tokens_generated,
            'tokens_per_second': round(tokens_per_second, 2),
            'vram_usage_gb': round(memory_after['gpu_allocated_gb'], 2)
        }
        
    def _run_single_benchmark(self, task_name: str, prompts: List[str]) -> Dict:
        print(f"  ğŸ“Š Benchmarking: {task_name}")
        results = []
        for i, p in enumerate(prompts):
            results.append(self.generate_response(p, f"{task_name} #{i+1}"))
            time.sleep(1) # Nghá»‰ giá»¯a cÃ¡c láº§n cháº¡y Ä‘á»ƒ á»•n Ä‘á»‹nh
        
        avg_time = np.mean([r['generation_time'] for r in results])
        avg_tps = np.mean([r['tokens_per_second'] for r in results])
        
        return {
            'task': task_name,
            'avg_generation_time': round(avg_time, 2),
            'avg_tokens_per_second': round(avg_tps, 2),
            'tests': results
        }

    def run_full_benchmark(self) -> Dict:
        """Cháº¡y toÃ n bá»™ bá»™ benchmark."""
        print("\nğŸš€ Starting EWQ Model Comprehensive Benchmark")
        print("=" * 60)
        
        tasks = {
            "Code Generation": [
                "Viáº¿t má»™t script Python sá»­ dá»¥ng thÆ° viá»‡n Pandas Ä‘á»ƒ Ä‘á»c file CSV cÃ³ tÃªn `sales_data.csv` vá»›i cÃ¡c cá»™t 'Date', 'Product', 'Revenue'. Script cáº§n tÃ­nh tá»•ng doanh thu theo tá»«ng sáº£n pháº©m vÃ  xuáº¥t káº¿t quáº£ ra má»™t file CSV má»›i cÃ³ tÃªn `revenue_by_product.csv`.",
                "Táº¡o má»™t component React functional báº±ng TypeScript tÃªn lÃ  `UserProfile`. Component nÃ y nháº­n vÃ o props lÃ  `name` (string), `age` (number), vÃ  `avatarUrl` (string), sau Ä‘Ã³ hiá»ƒn thá»‹ thÃ´ng tin nÃ y má»™t cÃ¡ch cÃ³ cáº¥u trÃºc."
            ],
            "Math Problem Solving": [
                "Má»™t bá»ƒ nÆ°á»›c cÃ³ hai vÃ²i. VÃ²i thá»© nháº¥t cháº£y má»™t mÃ¬nh thÃ¬ Ä‘áº§y bá»ƒ trong 4 giá». VÃ²i thá»© hai cháº£y má»™t mÃ¬nh thÃ¬ Ä‘áº§y bá»ƒ trong 6 giá». Náº¿u má»Ÿ cáº£ hai vÃ²i cÃ¹ng má»™t lÃºc khi bá»ƒ cáº¡n, há»i sau bao lÃ¢u thÃ¬ bá»ƒ sáº½ Ä‘áº§y? TrÃ¬nh bÃ y cÃ¡c bÆ°á»›c giáº£i chi tiáº¿t.",
                "Má»™t ngÆ°á»i gá»­i tiáº¿t kiá»‡m 500 triá»‡u Ä‘á»“ng vá»›i lÃ£i suáº¥t kÃ©p 6.5% má»—i nÄƒm. Há»i sau 5 nÄƒm, ngÆ°á»i Ä‘Ã³ sáº½ nháº­n Ä‘Æ°á»£c cáº£ vá»‘n láº«n lÃ£i lÃ  bao nhiÃªu tiá»n? YÃªu cáº§u trÃ¬nh bÃ y cÃ´ng thá»©c vÃ  cÃ¡c bÆ°á»›c tÃ­nh toÃ¡n."
            ],
            "Text Summarization": [
                "HÃ£y tÃ³m táº¯t Ä‘oáº¡n vÄƒn sau thÃ nh 3 Ã½ chÃ­nh: 'CÃ¡c cÃ´ng nghá»‡ thu giá»¯ carbon (Carbon Capture Technologies) Ä‘ang ná»•i lÃªn nhÆ° má»™t giáº£i phÃ¡p tiá»m nÄƒng trong cuá»™c chiáº¿n chá»‘ng biáº¿n Ä‘á»•i khÃ­ háº­u. CÃ¡c phÆ°Æ¡ng phÃ¡p chÃ­nh bao gá»“m thu giá»¯ sau Ä‘á»‘t chÃ¡y, thu giá»¯ trÆ°á»›c Ä‘á»‘t chÃ¡y vÃ  thu giá»¯ tá»« khÃ´ng khÃ­ trá»±c tiáº¿p (DAC). Máº·c dÃ¹ cÃ³ tiá»m nÄƒng lá»›n, cÃ´ng nghá»‡ nÃ y váº«n Ä‘á»‘i máº·t vá»›i nhá»¯ng thÃ¡ch thá»©c vá» chi phÃ­ váº­n hÃ nh cao, hiá»‡u quáº£ nÄƒng lÆ°á»£ng vÃ  váº¥n Ä‘á» lÆ°u trá»¯ carbon an toÃ n trong dÃ i háº¡n. CÃ¡c chÃ­nh phá»§ vÃ  táº­p Ä‘oÃ n lá»›n Ä‘ang Ä‘áº§u tÆ° hÃ ng tá»· USD vÃ o R&D Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ vÃ  giáº£m giÃ¡ thÃ nh, hy vá»ng biáº¿n nÃ³ thÃ nh má»™t cÃ´ng cá»¥ chá»§ chá»‘t vÃ o nÄƒm 2050.'",
                "TÃ³m táº¯t ngáº¯n gá»n nhá»¯ng sá»± kiá»‡n chÃ­nh vÃ  Ã½ nghÄ©a lá»‹ch sá»­ cá»§a cuá»™c CÃ¡ch máº¡ng CÃ´ng nghiá»‡p láº§n thá»© nháº¥t, táº­p trung vÃ o cÃ¡c phÃ¡t minh quan trá»ng vÃ  tÃ¡c Ä‘á»™ng cá»§a nÃ³ Ä‘áº¿n xÃ£ há»™i."
            ],
            "Creative Writing": [
                "Viáº¿t Ä‘oáº¡n má»Ÿ Ä‘áº§u cho má»™t cÃ¢u chuyá»‡n ngáº¯n thuá»™c thá»ƒ loáº¡i khoa há»c viá»…n tÆ°á»Ÿng, trong Ä‘Ã³ nhÃ¢n váº­t chÃ­nh lÃ  má»™t nhÃ  thá»±c váº­t há»c sá»‘ng trÃªn Sao Há»a, ngÆ°á»i vá»«a phÃ¡t hiá»‡n ra má»™t loÃ i cÃ¢y cÃ³ kháº£ nÄƒng giao tiáº¿p báº±ng Ã¡nh sÃ¡ng.",
                "Viáº¿t má»™t bÃ i vÄƒn ngáº¯n (khoáº£ng 150 tá»«) miÃªu táº£ cáº£nh má»™t phiÃªn chá»£ ná»•i trÃªn sÃ´ng Cá»­u Long vÃ o buá»•i sÃ¡ng sá»›m, táº­p trung vÃ o Ã¢m thanh, mÃ u sáº¯c vÃ  mÃ¹i vá»‹ Ä‘áº·c trÆ°ng."
            ],
            "Question Answering": [
                "Giáº£i thÃ­ch sá»± khÃ¡c biá»‡t cÆ¡ báº£n giá»¯a nÄƒng lÆ°á»£ng háº¡t nhÃ¢n phÃ¢n háº¡ch (nuclear fission) vÃ  tá»•ng há»£p háº¡t nhÃ¢n (nuclear fusion). Loáº¡i nÃ o hiá»‡n Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nhÃ  mÃ¡y Ä‘iá»‡n?",
                "Con Ä‘Æ°á»ng tÆ¡ lá»¥a lÃ  gÃ¬ vÃ  nÃ³ cÃ³ vai trÃ² quan trá»ng nhÆ° tháº¿ nÃ o Ä‘á»‘i vá»›i sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c ná»n vÄƒn minh cá»• Ä‘áº¡i?"
            ],
            "Language Translation & Nuance": [
                "Dá»‹ch Ä‘oáº¡n vÄƒn sau sang tiáº¿ng Viá»‡t, chÃº Ã½ giá»¯ vÄƒn phong chuyÃªn nghiá»‡p: 'Our team is conducting a comprehensive due diligence process to assess the viability of the potential acquisition. Key performance indicators (KPIs) and financial statements are under rigorous scrutiny.'",
                "Giáº£i thÃ­ch Ã½ nghÄ©a vÃ  tÃ¬m cÃ¢u thÃ nh ngá»¯ tiáº¿ng Anh cÃ³ nghÄ©a tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¢u 'NÆ°á»›c Ä‘áº¿n chÃ¢n má»›i nháº£y'."
            ],
            "Reasoning & Logic": [
                "CÃ³ ba chiáº¿c há»™p. Má»™t há»™p chá»©a toÃ n bi Ä‘á», má»™t há»™p chá»©a toÃ n bi xanh, vÃ  má»™t há»™p chá»©a láº«n lá»™n cáº£ bi Ä‘á» vÃ  bi xanh. Cáº£ ba há»™p Ä‘á»u bá»‹ dÃ¡n nhÃ£n sai. Báº¡n chá»‰ Ä‘Æ°á»£c phÃ©p láº¥y ra má»™t viÃªn bi tá»« má»™t há»™p duy nháº¥t (khÃ´ng Ä‘Æ°á»£c nhÃ¬n vÃ o bÃªn trong). LÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c ná»™i dung cá»§a cáº£ ba há»™p? HÃ£y giáº£i thÃ­ch quÃ¡ trÃ¬nh suy luáº­n cá»§a báº¡n.",
                "Má»™t nghiÃªn cá»©u cho tháº¥y nhá»¯ng thÃ nh phá»‘ cÃ³ nhiá»u cá»­a hÃ ng kem nháº¥t cÅ©ng cÃ³ tá»· lá»‡ tá»™i pháº¡m cao nháº¥t. CÃ³ pháº£i Äƒn kem gÃ¢y ra tá»™i pháº¡m khÃ´ng? HÃ£y giáº£i thÃ­ch vá» má»‘i tÆ°Æ¡ng quan vÃ  quan há»‡ nhÃ¢n quáº£ (correlation vs. causation) trong trÆ°á»ng há»£p nÃ y."
            ],
            "Technical Explanation": [
                "HÃ£y giáº£i thÃ­ch cho má»™t ngÆ°á»i khÃ´ng rÃ nh vá» cÃ´ng nghá»‡ vá» khÃ¡i niá»‡m 'Äiá»‡n toÃ¡n Ä‘Ã¡m mÃ¢y' (Cloud Computing) lÃ  gÃ¬. Sá»­ dá»¥ng vÃ­ dá»¥ vá» Google Drive hoáº·c iCloud Ä‘á»ƒ minh há»a.",
                "Giáº£i thÃ­ch má»™t cÃ¡ch Ä‘Æ¡n giáº£n quÃ¡ trÃ¬nh quang há»£p á»Ÿ thá»±c váº­t diá»…n ra nhÆ° tháº¿ nÃ o vÃ  táº¡i sao nÃ³ láº¡i quan trá»ng Ä‘á»‘i vá»›i sá»± sá»‘ng trÃªn TrÃ¡i Äáº¥t."
            ]
        }

        benchmark_results = []
        for name, prompts in tasks.items():
            benchmark_results.append(self._run_single_benchmark(name, prompts))
            gc.collect()
            torch.cuda.empty_cache()

        overall_tps = [r['avg_tokens_per_second'] for r in benchmark_results]
        overall_stats = {
            'avg_token_speed': round(np.mean(overall_tps), 2) if overall_tps else 0,
        }
        
        return {
            'model_hash': self.model_hash,
            'timestamp': datetime.now().isoformat(),
            'overall_stats': overall_stats,
            'benchmark_results': benchmark_results
        }

    def save_and_print_summary(self, results: Dict):
        """LÆ°u vÃ  in káº¿t quáº£ benchmark."""
        stats = results['overall_stats']
        print("\n" + "="*60 + "\nğŸ“Š EWQ MODEL BENCHMARK RESULTS SUMMARY\n" + "="*60)
        print(f"ğŸ¯ Model Hash: {results['model_hash']}")
        print(f"ğŸ“ˆ Average Token Speed (Overall): {stats['avg_token_speed']:.2f} tokens/sec")
        
        print("\nğŸ“‹ Task Performance:")
        for result in results['benchmark_results']:
            vram = result['tests'][0]['vram_usage_gb'] if result['tests'] else 'N/A'
            print(f"  - {result['task']:<25}: {result['avg_tokens_per_second']:.2f} tok/s @ {vram} GB VRAM")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ewq_benchmark_{self.model_hash}_{timestamp}.json"
        filepath = Path("benchmark_results") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Full detailed results saved to: {filepath}")

# === HÃ€M MAIN Äá»‚ CHáº Y BENCHMARK ===
def main():
    print("ğŸ” EWQ Model Benchmark System (v4 - VRAM Optimized with Full Logging)")
    print("=" * 60)
    
    try:
        model_config = {'base_model': MODEL_ID, 'entropy_factor': ENTROPY_THRESHOLD_FACTOR, 'quant_method': 'ewq-bitsandbytes'}
        model_hash = get_model_hash(MODEL_ID, model_config)
        print(f"ğŸ”‘ Model Config Hash: {model_hash}")
        
        if not check_quantized_model_exists(model_hash):
            print(f"âŒ Error: Quantization plan not found for hash '{model_hash}'.")
            print("ğŸ’¡ Please run the 'main_cache_model.py' (plan generator) script first.")
            return
        
        print("âœ… Found quantization plan. Proceeding to load and quantize model.")
        model, tokenizer = load_quantized_model(MODEL_ID, model_hash)
        
        if model is None or tokenizer is None:
            print("âŒ Failed to load and quantize model!"); return
        
        print("âœ… Quantized model is on GPU and ready for benchmarking!")
        
        benchmark_suite = BenchmarkSuite(model, tokenizer, model_hash)
        results = benchmark_suite.run_full_benchmark()
        benchmark_suite.save_and_print_summary(results)
        
        print(f"\nğŸŠ Benchmark completed successfully!")
        
    except Exception as e:
        print(f"âŒ An unexpected error occurred during benchmark: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()