### üá¨üáß English version is available below.

---

# L∆∞·ª£ng t·ª≠ h√≥a theo Entropy (EWQ): T√°i hi·ªán v√† Ki·ªÉm th·ª≠ To√†n di·ªán (Phi√™n b·∫£n Tensor-wise)

D·ª± √°n n√†y l√† m·ªôt n·ªó l·ª±c ƒë·ªôc l·∫≠p nh·∫±m t√°i hi·ªán v√† c·∫£i ti·∫øn ph∆∞∆°ng ph√°p **L∆∞·ª£ng t·ª≠ h√≥a theo Entropy (Entropy-Weighted Quantization - EWQ)** ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t trong b√†i b√°o khoa h·ªçc tr√™n [arXiv:2503.04704v2](https://arxiv.org/html/2503.04704v2). Phi√™n b·∫£n n√†y ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p t·ª´ ph√¢n t√≠ch theo block l√™n **ph√¢n t√≠ch theo t·ª´ng tensor**, cho ph√©p t·ªëi ∆∞u h√≥a ·ªü m·ª©c ƒë·ªô chi ti·∫øt v√† hi·ªáu qu·∫£ h∆°n.

M·ª•c ti√™u c·ªßa d·ª± √°n:
1.  **T√°i hi·ªán v√† c·∫£i ti·∫øn thu·∫≠t to√°n EWQ c·ªët l√µi** ƒë·ªÉ t·∫°o ra m·ªôt k·∫ø ho·∫°ch l∆∞·ª£ng t·ª≠ h√≥a (quantization plan) t√πy ch·ªânh ·ªü c·∫•p ƒë·ªô tensor cho c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs).
2.  **X√¢y d·ª±ng m·ªôt b·ªô ki·ªÉm th·ª≠ (benchmark suite) to√†n di·ªán** ƒë·ªÉ ƒë√°nh gi√° m·ªôt c√°ch kh√°ch quan v√† nghi√™m ng·∫∑t hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p EWQ so v·ªõi c√°c k·ªπ thu·∫≠t l∆∞·ª£ng t·ª≠ h√≥a ti√™u chu·∫©n.
3.  **X√°c th·ª±c** li·ªáu ph∆∞∆°ng ph√°p EWQ-Tensorwise c√≥ th·ª±c s·ª± t·∫°o ra m·ªôt model c√¢n b·∫±ng v∆∞·ª£t tr·ªôi v·ªÅ ch·∫•t l∆∞·ª£ng, hi·ªáu su·∫•t v√† vi·ªác s·ª≠ d·ª•ng t√†i nguy√™n hay kh√¥ng.

## Thu·∫≠t to√°n L∆∞·ª£ng t·ª≠ h√≥a theo Entropy (Tensor-wise) ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?

C·ªët l√µi c·ªßa d·ª± √°n l√† script `create_quantization_plan_tensorwise.py`. Thay v√¨ √°p d·ª•ng m·ªôt ph∆∞∆°ng ph√°p l∆∞·ª£ng t·ª≠ h√≥a ƒë·ªìng nh·∫•t ho·∫∑c theo t·ª´ng block, thu·∫≠t to√°n EWQ-Tensorwise th·ª±c hi·ªán m·ªôt c√°ch ti·∫øp c·∫≠n c·ª±c k·ª≥ chi ti·∫øt:

1.  **Ph√¢n t√≠ch tr√™n CPU:** To√†n b·ªô model g·ªëc ƒë∆∞·ª£c t·∫£i l√™n CPU ƒë·ªÉ ph√¢n t√≠ch, gi√∫p ti·∫øt ki·ªám VRAM v√† cho ph√©p x·ª≠ l√Ω c√°c model c·ª±c l·ªõn.
2.  **Ph√¢n t√≠ch Entropy v√† T·∫ßm quan tr·ªçng cho t·ª´ng Tensor:** Thu·∫≠t to√°n l·∫∑p qua **t·ª´ng tensor tr·ªçng s·ªë** (v√≠ d·ª•: `layers.0.self_attn.q_proj.weight`) trong to√†n b·ªô model.
    *   **T√≠nh to√°n Entropy:** **Shannon entropy** ƒë∆∞·ª£c t√≠nh cho m·ªói tensor, ƒë√≥ng vai tr√≤ l√† m·ªôt th∆∞·ªõc ƒëo v·ªÅ "m·ª©c ƒë·ªô ph·ª©c t·∫°p" hay "l∆∞·ª£ng th√¥ng tin" m√† tensor ƒë√≥ n·∫Øm gi·ªØ.
    *   **G√°n Tr·ªçng s·ªë Quan tr·ªçng:** M·ªói tensor ƒë∆∞·ª£c g√°n m·ªôt **h·ªá s·ªë quan tr·ªçng (`TENSOR_IMPORTANCE`)** d·ª±a tr√™n t√™n v√† vai tr√≤ c·ªßa n√≥ (v√≠ d·ª•: c√°c tensor embedding v√† output ƒë∆∞·ª£c coi l√† quan tr·ªçng nh·∫•t).
3.  **X√°c ƒë·ªãnh Ng∆∞·ª°ng Th√≠ch ·ª©ng:** Thu·∫≠t to√°n ph√¢n t√≠ch s·ª± ph√¢n b·ªï entropy c·ªßa to√†n b·ªô c√°c tensor v√† t·∫°o ra c√°c **ng∆∞·ª°ng ƒë·ªông** d·ª±a tr√™n gi√° tr·ªã trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n.
4.  **Logic ra Quy·∫øt ƒë·ªãnh ƒêa y·∫øu t·ªë:** D·ª±a tr√™n m·ªôt t·ªï h·ª£p c√°c y·∫øu t·ªë c·ªßa m·ªói tensor, m·ªôt quy·∫øt ƒë·ªãnh l∆∞·ª£ng t·ª≠ h√≥a ƒë∆∞·ª£c ƒë∆∞a ra:
    *   **Entropy** c·ªßa tensor.
    *   **ƒê·ªô quan tr·ªçng** ƒë∆∞·ª£c g√°n tr∆∞·ªõc.
    *   **K√≠ch th∆∞·ªõc** c·ªßa tensor (c√°c tensor r·∫•t nh·ªè s·∫Ω ƒë∆∞·ª£c gi·ªØ nguy√™n).
    *   **Quy t·∫Øc c·ª©ng:** C√°c tensor t·ªëi quan tr·ªçng (v√≠ d·ª•: `output.weight`) lu√¥n ƒë∆∞·ª£c gi·ªØ ·ªü ƒë·ªô ch√≠nh x√°c cao nh·∫•t (FP16).
5.  **K·∫øt qu·∫£:** Qu√° tr√¨nh n√†y t·∫°o ra m·ªôt file `quant_plan_*.json` c·ª±c k·ª≥ chi ti·∫øt, ch·ª©a k·∫ø ho·∫°ch l∆∞·ª£ng t·ª≠ h√≥a cho t·ª´ng tensor ri√™ng l·∫ª. ƒêi k√®m v·ªõi ƒë√≥ l√† m·ªôt file k·ªãch b·∫£n shell `quantize_command_*.sh` ƒë·ªÉ t·ª± ƒë·ªông t·∫°o ra model GGUF t√πy ch·ªânh t·ª´ k·∫ø ho·∫°ch n√†y.

## H·ªá th·ªëng Ki·ªÉm th·ª≠ (Benchmark)

ƒê·ªÉ ch·ª©ng minh gi√° tr·ªã c·ªßa EWQ, m·ªôt b·ªô benchmark to√†n di·ªán ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng ƒë·ªÉ so s√°nh c√°c phi√™n b·∫£n model kh√°c nhau tr√™n nhi·ªÅu kh√≠a c·∫°nh:

**C√°c phi√™n b·∫£n ƒë∆∞·ª£c so s√°nh:**
*   **Original (FP16):** Model g·ªëc ch∆∞a qua l∆∞·ª£ng t·ª≠ h√≥a.
*   **Standard Q4_K_M (GGUF):** L∆∞·ª£ng t·ª≠ h√≥a 4-bit ti√™u chu·∫©n.
*   **Standard Q8_0 (GGUF):** L∆∞·ª£ng t·ª≠ h√≥a 8-bit ti√™u chu·∫©n.
*   **EWQ (bitsandbytes - Blockwise):** Plan EWQ theo block √°p d·ª•ng qua `bitsandbytes`.
*   **EWQ (GGUF - Blockwise):** Plan EWQ theo block ƒë·ªÉ t·∫°o file GGUF.
*   **EWQ (GGUF - Tensorwise):** Plan EWQ theo tensor ƒë·ªÉ t·∫°o file GGUF (phi√™n b·∫£n m·ªõi nh·∫•t).

**C√°c ch·ªâ s·ªë ƒë∆∞·ª£c ƒëo l∆∞·ªùng:**
*   **Ch·∫•t l∆∞·ª£ng Model:** MMLU, BLEU, ROUGE-1/2/L.
*   **Hi·ªáu su·∫•t:** T·ªëc ƒë·ªô sinh token (tokens/gi√¢y).
*   **T√†i nguy√™n:** M·ª©c s·ª≠ d·ª•ng VRAM (GB).

## K·∫øt qu·∫£ Benchmark v√† Ph√¢n t√≠ch

ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t, ch·ª©ng minh hi·ªáu qu·∫£ c·ªßa c√°c ph∆∞∆°ng ph√°p.

### B·∫£ng T·ªïng h·ª£p K·∫øt qu·∫£ (Model: Qwen3-8B)

| Phi√™n b·∫£n Model | VRAM (GB) | % Œî VRAM | T·ªëc ƒë·ªô (tok/s) | % Œî T·ªëc ƒë·ªô | MMLU (%) | % Œî MMLU | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **Original (FP16)** | 15.26 | *Baseline* | 47.04 | *Baseline* | 69.98 | *Baseline* | - | - | - | 0.1783 |
| **Standard Q4_K_M (GGUF)** | **6.40** | **-58.1%** | **124.14** | **+163.9%**| 69.32 | -0.9% | 0.0329 | 0.2600 | 0.0767 | 0.1564 |
| **Standard Q8_0 (GGUF)** | 9.54 | -37.5% | 85.83 | +82.5% | 70.27 | +0.4% | 0.0306 | 0.2540 | 0.0746 | 0.1515 |
| **EWQ (GGUF - Blockwise)** | 8.54 | -44.0% | 95.18 | +102.3% | 70.05 | +0.1% | 0.0322 | 0.2544 | 0.0724 | 0.1548 |
| **EWQ (GGUF - Tensorwise)** | 7.55 | -50.5% | 106.62 | +126.7% | 70.09 | +0.2% | 0.0303 | 0.2513 | 0.0712 | 0.1518 |
| **EWQ (bitsandbytes - Blockwise)** | 9.64 | -36.8% | 43.32 | -7.9% | **70.30** | **+0.5%** | **0.0487** | **0.3148** | **0.0909** | **0.1800** |

*(**In ƒë·∫≠m** l√† gi√° tr·ªã t·ªët nh·∫•t trong t·ª´ng h·∫°ng m·ª•c. `% Œî` l√† ph·∫ßn trƒÉm thay ƒë·ªïi so v·ªõi b·∫£n g·ªëc FP16)*

### Ph√¢n t√≠ch

1.  **Ch·∫•t l∆∞·ª£ng Model (MMLU & ROUGE):**
    *   **Q4_K_M** l√† phi√™n b·∫£n GGUF duy nh·∫•t cho th·∫•y s·ª± s·ª•t gi·∫£m nh·∫π v·ªÅ ƒëi·ªÉm MMLU (-0.9%), cho th·∫•y vi·ªác l∆∞·ª£ng t·ª≠ h√≥a m·∫°nh tay ƒë·ªìng nh·∫•t c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn kh·∫£ nƒÉng suy lu·∫≠n.
    *   T·∫•t c·∫£ c√°c phi√™n b·∫£n **EWQ v√† Q8_0 ƒë·ªÅu b·∫£o to√†n ho·∫∑c th·∫≠m ch√≠ c·∫£i thi·ªán nh·∫π** ƒëi·ªÉm MMLU so v·ªõi b·∫£n g·ªëc, ch·ª©ng t·ªè c√°c ph∆∞∆°ng ph√°p l∆∞·ª£ng t·ª≠ h√≥a tinh vi h∆°n c√≥ hi·ªáu qu·∫£ trong vi·ªác gi·ªØ l·∫°i "tr√≠ th√¥ng minh" c·ªßa model.
    *   V·ªÅ ch·∫•t l∆∞·ª£ng sinh vƒÉn b·∫£n (ROUGE), phi√™n b·∫£n **EWQ (bitsandbytes)** cho k·∫øt qu·∫£ v∆∞·ª£t tr·ªôi, c√≥ th·ªÉ do c√°ch th∆∞ vi·ªán n√†y x·ª≠ l√Ω c√°c ph√©p to√°n. Tuy nhi√™n, c√°c phi√™n b·∫£n GGUF kh√°c ƒë·ªÅu cho k·∫øt qu·∫£ kh√° t∆∞∆°ng ƒë·ªìng nhau.

2.  **T√†i nguy√™n v√† Hi·ªáu su·∫•t (VRAM & T·ªëc ƒë·ªô):**
    *   **Standard Q4_K_M** l√† phi√™n b·∫£n **nhanh nh·∫•t v√† nh·∫π nh·∫•t**, nh∆∞ng ph·∫£i tr·∫£ gi√° b·∫±ng vi·ªác s·ª•t gi·∫£m ch·∫•t l∆∞·ª£ng.
    *   Ph∆∞∆°ng ph√°p **EWQ (GGUF - Tensorwise)** t·ªèa s√°ng r·ª±c r·ª° nh∆∞ m·ªôt **nh√† v√¥ ƒë·ªãch v·ªÅ s·ª± c√¢n b·∫±ng**. N√≥ mang l·∫°i m·ªôt b∆∞·ªõc nh·∫£y v·ªçt v·ªÅ hi·ªáu su·∫•t so v·ªõi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p kh√°c (ngo·∫°i tr·ª´ Q4).
    *   So v·ªõi **Standard Q8_0**, phi√™n b·∫£n **EWQ-Tensorwise** v∆∞·ª£t tr·ªôi v·ªÅ m·ªçi m·∫∑t: **nh·∫π h∆°n 21%** (ti·∫øt ki·ªám 2GB VRAM), **nhanh h∆°n 24%**, trong khi ch·∫•t l∆∞·ª£ng MMLU g·∫ßn nh∆∞ t∆∞∆°ng ƒë∆∞∆°ng.
    *   So v·ªõi ch√≠nh phi√™n b·∫£n **EWQ-Blockwise**, vi·ªác chuy·ªÉn sang **Tensorwise** l√† m·ªôt c·∫£i ti·∫øn l·ªõn: **nh·∫π h∆°n 12%** (ti·∫øt ki·ªám 1GB VRAM) v√† **nhanh h∆°n 12%**.

## K·∫øt lu·∫≠n

D·ª± √°n ƒë√£ t√°i hi·ªán v√† c·∫£i ti·∫øn th√†nh c√¥ng ph∆∞∆°ng ph√°p L∆∞·ª£ng t·ª≠ h√≥a theo Entropy l√™n m·ª©c ƒë·ªô tensor. B·ªô ki·ªÉm th·ª≠ nghi√™m ng·∫∑t ƒë√£ ch·ª©ng minh m·ªôt c√°ch thuy·∫øt ph·ª•c t√≠nh hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi c·ªßa n√≥.

**K·∫øt qu·∫£ cho th·∫•y r√µ r√†ng r·∫±ng vi·ªác s·ª≠ d·ª•ng plan EWQ-Tensorwise ƒë·ªÉ t·∫°o ra m·ªôt file GGUF t√πy ch·ªânh ƒë√£ t·∫°o ra m·ªôt phi√™n b·∫£n model c·ª±c k·ª≥ c√¢n b·∫±ng v√† t·ªëi ∆∞u. N√≥ l√† s·ª± l·ª±a ch·ªçn t·ªët nh·∫•t cho nh·ªØng ai t√¨m ki·∫øm "ƒëi·ªÉm ng·ªçt" ho√†n h·∫£o gi·ªØa hi·ªáu su·∫•t, y√™u c·∫ßu t√†i nguy√™n v√† ch·∫•t l∆∞·ª£ng, v∆∞·ª£t qua c·∫£ ph∆∞∆°ng ph√°p l∆∞·ª£ng t·ª≠ h√≥a 8-bit ti√™u chu·∫©n v√† c√°c phi√™n b·∫£n EWQ c≈© h∆°n.**

---

# Entropy-Weighted Quantization (EWQ): Comprehensive Re-implementation and Benchmark (Tensor-wise Edition)

This project is an independent effort to re-implement and enhance the **Entropy-Weighted Quantization (EWQ)** method proposed in the scientific paper [arXiv:2503.04704v2](https://arxiv.org/html/2503.04704v2). This version has been upgraded from block-wise analysis to **tensor-wise analysis**, enabling more granular and effective optimization.

Project Goals:
1.  **Re-implement and enhance the core EWQ algorithm** to generate a custom, tensor-level quantization plan for Large Language Models (LLMs).
2.  **Build a comprehensive benchmark suite** to objectively and rigorously evaluate the effectiveness of the EWQ method against standard quantization techniques.
3.  **Validate** whether the EWQ-Tensorwise method truly produces a superiorly balanced model in terms of quality, performance, and resource consumption.

## Benchmark Results and Analysis

This is the most crucial section, demonstrating the effectiveness of the different methods.

### Summary Table (Model: Qwen3-8B)

| Model Version | VRAM (GB) | % Œî VRAM | Speed (tok/s) | % Œî Speed | MMLU (%) | % Œî MMLU | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **Original (FP16)** | 15.26 | *Baseline* | 47.04 | *Baseline* | 69.98 | *Baseline* | - | - | - | 0.1783 |
| **Standard Q4_K_M (GGUF)** | **6.40** | **-58.1%** | **124.14** | **+163.9%**| 69.32 | -0.9% | 0.0329 | 0.2600 | 0.0767 | 0.1564 |
| **Standard Q8_0 (GGUF)** | 9.54 | -37.5% | 85.83 | +82.5% | 70.27 | +0.4% | 0.0306 | 0.2540 | 0.0746 | 0.1515 |
| **EWQ (GGUF - Blockwise)** | 8.54 | -44.0% | 95.18 | +102.3% | 70.05 | +0.1% | 0.0322 | 0.2544 | 0.0724 | 0.1548 |
| **EWQ (GGUF - Tensorwise)** | 7.55 | -50.5% | 106.62 | +126.7% | 70.09 | +0.2% | 0.0303 | 0.2513 | 0.0712 | 0.1518 |
| **EWQ (bitsandbytes - Blockwise)** | 9.64 | -36.8% | 43.32 | -7.9% | **70.30** | **+0.5%** | **0.0487** | **0.3148** | **0.0909** | **0.1800** |

*(**Bold** indicates the best value in each category. `% Œî` is the percentage change relative to the FP16 original.)*

### Analysis

1.  **Model Quality (MMLU & ROUGE):**
    *   **Q4_K_M** is the only GGUF version that shows a slight drop in its MMLU score (-0.9%), suggesting that aggressive, uniform quantization impacts reasoning capabilities.
    *   All **EWQ and Q8_0 versions maintained or even slightly improved** the MMLU score compared to the original, proving that more sophisticated quantization methods are effective at preserving the model's "intelligence."
    *   For text generation quality (ROUGE), the **EWQ (bitsandbytes)** version shows superior results, possibly due to the library's operational handling. However, the other GGUF versions perform quite similarly to each other.

2.  **Resources and Performance (VRAM & Speed):**
    *   **Standard Q4_K_M** is the **fastest and lightest** version, but it comes at the cost of reduced quality.
    *   The **EWQ (GGUF - Tensorwise)** method shines as the **champion of balance**. It delivers a leap in performance over all other methods (except Q4).
    *   Compared to **Standard Q8_0**, the **EWQ-Tensorwise** version is superior in every aspect: **21% lighter** (saving 2GB of VRAM), **24% faster**, with a virtually identical MMLU score.
    *   Compared to its **EWQ-Blockwise** predecessor, the move to **Tensorwise** is a major improvement: **12% lighter** (saving 1GB of VRAM) and **12% faster**.

## Conclusion

This project has successfully re-implemented and advanced the Entropy-Weighted Quantization method to the tensor level. The rigorous benchmark suite has convincingly proven its outstanding effectiveness.

**The results clearly demonstrate that using a Tensor-wise EWQ plan to create a custom GGUF file produces an extremely balanced and optimized model. It is the best choice for those seeking the perfect "sweet spot" between performance, resource requirements, and quality, outperforming both standard 8-bit quantization and older EWQ versions.**