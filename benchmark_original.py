#!/usr/bin/env python3
"""
Original Model Benchmark System - WITHOUT EWQ Quantization

Script n√†y d√πng ƒë·ªÉ benchmark model g·ªëc (kh√¥ng l∆∞·ª£ng t·ª≠ h√≥a) ƒë·ªÉ so s√°nh v·ªõi k·∫øt qu·∫£ EWQ.
Bao g·ªìm t·∫•t c·∫£ c√°c b√†i test: MMLU, Perplexity v√† Generation tasks.

Quy tr√¨nh:
1. T·∫£i model g·ªëc l√™n GPU v·ªõi precision cao nh·∫•t c√≥ th·ªÉ.
2. Ch·∫°y to√†n b·ªô b·ªô benchmark v·ªõi ƒë·∫ßy ƒë·ªß logging.
3. L∆∞u k·∫øt qu·∫£ ƒë·ªÉ so s√°nh v·ªõi EWQ benchmark.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import psutil
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import gc
import os
import warnings
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

warnings.filterwarnings("ignore")

# === C·∫•u h√¨nh ===
MODEL_ID = "unsloth/Meta-Llama-3.1-8B-Instruct"
MODEL_CACHE_DIR = "./models"

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# === MMLU Test Questions (Sample subset for efficiency) ===
MMLU_QUESTIONS = {
    "abstract_algebra": [
        {
            "question": "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.",
            "choices": ["0", "4", "2", "6"],
            "answer": 1
        },
        {
            "question": "Let p = (1, 2, 5, 4)(2, 3) in S_5. Find the index of <p> in S_5.",
            "choices": ["8", "2", "24", "120"],
            "answer": 2
        }
    ],
    "anatomy": [
        {
            "question": "Which of the following is the body cavity that contains the pituitary gland?",
            "choices": ["Abdominal", "Cranial", "Pleural", "Spinal"],
            "answer": 1
        },
        {
            "question": "What is the embryological origin of the hyoid bone?",
            "choices": ["The first pharyngeal arch", "The first and second pharyngeal arches", "The second pharyngeal arch", "The second and third pharyngeal arches"],
            "answer": 3
        }
    ],
    "astronomy": [
        {
            "question": "How long does it take for light to travel from the Sun to the Earth?",
            "choices": ["8 minutes", "1 hour", "1 day", "1 year"],
            "answer": 0
        },
        {
            "question": "Where do most short-period comets come from and how do we know?",
            "choices": ["The Kuiper belt; short period comets tend to be in the plane of the solar system just like the Kuiper belt.", "The Kuiper belt; short period comets tend to come from random directions indicating a spherical distribution of comets called the Kuiper belt.", "The Oort cloud; short period comets tend to be in the plane of the solar system just like the Oort cloud.", "The Oort cloud; short period comets tend to come from random directions indicating a spherical distribution of comets called the Oort cloud."],
            "answer": 0
        }
    ],
    "business_ethics": [
        {
            "question": "According to Kant's categorical imperative, we should:",
            "choices": ["Act only according to that maxim whereby you can at the same time will that it should become a universal law.", "Act to maximize happiness for the greatest number of people.", "Act only according to that maxim which you can at the same time will to be a universal law of nature.", "Act to benefit yourself, as long as you don't harm others."],
            "answer": 0
        },
        {
            "question": "What is the difference between a stakeholder and a shareholder?",
            "choices": ["Stakeholders own shares in the company, while shareholders have an interest in the company.", "Stakeholders have an interest in the company, while shareholders own shares in the company.", "Stakeholders and shareholders are the same thing.", "Stakeholders are employees, while shareholders are customers."],
            "answer": 1
        }
    ],
    "clinical_knowledge": [
        {
            "question": "Glycolysis is the name given to the pathway involving the conversion of:",
            "choices": ["glycogen to glucose-1-phosphate.", "glycogen or starch to fructose.", "glycogen or starch to glucose or glucose-1-phosphate.", "glucose to pyruvate or lactate."],
            "answer": 3
        },
        {
            "question": "A patient has been on the operating table for four hours. How long may it take for any pressure damage to be reversible?",
            "choices": ["30 minutes.", "2 hours.", "12 hours.", "24 hours."],
            "answer": 2
        }
    ]
}

# === Perplexity Test Passages ===
PERPLEXITY_PASSAGES = [
    "The quick brown fox jumps over the lazy dog. This pangram contains every letter of the English alphabet at least once.",
    "In the beginning was the Word, and the Word was with God, and the Word was God. He was in the beginning with God.",
    "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from and make predictions on data.",
    "The mitochondria is the powerhouse of the cell, responsible for producing ATP through cellular respiration.",
    "Climate change refers to long-term shifts in temperatures and weather patterns, primarily caused by human activities since the 1800s.",
    "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles.",
    "The Internet has revolutionized communication, commerce, and access to information, connecting billions of people worldwide.",
    "Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to create oxygen and energy in the form of sugar."
]

def get_model_hash(model_id: str) -> str:
    """T·∫°o hash cho model g·ªëc (kh√¥ng quantization)."""
    config_str = f"{model_id}-original-no-quantization"
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]

def load_original_model(model_id: str) -> Tuple[Optional[nn.Module], Optional[AutoTokenizer]]:
    """T·∫£i model g·ªëc v·ªõi precision cao nh·∫•t c√≥ th·ªÉ."""
    print("  üì• Loading original model to GPU...")
    
    # Ki·ªÉm tra VRAM available
    if torch.cuda.is_available():
        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  üíæ Available VRAM: {total_vram:.2f} GB")
        
        # Ch·ªçn precision d·ª±a tr√™n VRAM
        if total_vram >= 16:
            torch_dtype = torch.float32
            print("  üéØ Using float32 precision (highest quality)")
        elif total_vram >= 12:
            torch_dtype = torch.float16
            print("  üéØ Using float16 precision (balanced)")
        else:
            torch_dtype = torch.float16
            print("  ‚ö†Ô∏è Using float16 precision (limited VRAM)")
            
        device_map = "auto"
    else:
        print("  ‚ö†Ô∏è WARNING: No CUDA device found. Model will run on CPU.")
        torch_dtype = torch.float32
        device_map = "cpu"
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=MODEL_CACHE_DIR,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
            low_cpu_mem_usage=True if torch.cuda.is_available() else False
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            cache_dir=MODEL_CACHE_DIR,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("  ‚úÖ Original model loaded successfully!")
        return model, tokenizer
        
    except Exception as e:
        print(f"  ‚ùå Failed to load original model: {e}")
        return None, None


# === ORIGINAL MODEL BENCHMARK SUITE ===

class OriginalModelBenchmarkSuite:
    def __init__(self, model, tokenizer, model_hash: str):
        self.model = model
        self.tokenizer = tokenizer
        self.model_hash = model_hash
        self.device = next(model.parameters()).device
        self.model_precision = str(next(model.parameters()).dtype)
        
    def get_memory_usage(self) -> Dict[str, float]:
        """L·∫•y th√¥ng tin s·ª≠ d·ª•ng b·ªô nh·ªõ (RAM & VRAM)."""
        process = psutil.Process()
        ram_gb = process.memory_info().rss / 1024**3
        
        gpu_allocated_gb = 0
        gpu_reserved_gb = 0
        if torch.cuda.is_available():
            gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3
            gpu_reserved_gb = torch.cuda.memory_reserved() / 1024**3
        
        return {
            'ram_gb': ram_gb, 
            'gpu_allocated_gb': gpu_allocated_gb,
            'gpu_reserved_gb': gpu_reserved_gb
        }
    
    def generate_response(self, prompt: str, task_name: str = "") -> Dict:
        """Sinh response v√† ƒëo l∆∞·ªùng hi·ªáu nƒÉng chi ti·∫øt."""
        print(f"    üîÑ Generating response for: {task_name}")
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        memory_before = self.get_memory_usage()
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
              **inputs,
              max_new_tokens=512,
              do_sample=True,
              temperature=0.6,
              top_p=0.9,
              repetition_penalty=1.1,
              pad_token_id=self.tokenizer.pad_token_id,
              eos_token_id=self.tokenizer.eos_token_id,
              use_cache=True 
            )
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        memory_after = self.get_memory_usage()
        
        input_length = inputs['input_ids'].shape[1]
        response_tokens = outputs[0][input_length:]
        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
        
        generation_time = end_time - start_time
        tokens_generated = len(response_tokens)
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            'prompt': prompt,
            'response': response,
            'generation_time': round(generation_time, 2),
            'tokens_generated': tokens_generated,
            'tokens_per_second': round(tokens_per_second, 2),
            'vram_allocated_gb': round(memory_after['gpu_allocated_gb'], 2),
            'vram_reserved_gb': round(memory_after['gpu_reserved_gb'], 2)
        }

    def run_mmlu_test(self) -> Dict:
        """Ch·∫°y MMLU (Massive Multitask Language Understanding) test."""
        print("  üß† Running MMLU Test...")
        
        correct_answers = 0
        total_questions = 0
        subject_results = {}
        
        for subject, questions in MMLU_QUESTIONS.items():
            print(f"    üìö Testing subject: {subject}")
            subject_correct = 0
            
            for i, q in enumerate(questions):
                # Format c√¢u h·ªèi theo chu·∫©n MMLU
                choices_text = "\n".join([f"{chr(65+j)}. {choice}" for j, choice in enumerate(q['choices'])])
                prompt = f"Question: {q['question']}\n{choices_text}\nAnswer:"
                
                # Generate response
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=5,  # Ch·ªâ c·∫ßn 1 k√Ω t·ª± cho ƒë√°p √°n
                        do_sample=False,   # Greedy decoding cho t√≠nh nh·∫•t qu√°n
                        temperature=0.1,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
                
                # Ki·ªÉm tra ƒë√°p √°n
                predicted_letter = response[0].upper() if response and response[0].upper() in 'ABCD' else 'X'
                correct_letter = chr(65 + q['answer'])
                
                if predicted_letter == correct_letter:
                    subject_correct += 1
                    correct_answers += 1
                
                total_questions += 1
                
                print(f"      Q{i+1}: Predicted={predicted_letter}, Correct={correct_letter}, {'‚úì' if predicted_letter == correct_letter else '‚úó'}")
            
            subject_accuracy = subject_correct / len(questions) * 100
            subject_results[subject] = {
                'correct': subject_correct,
                'total': len(questions),
                'accuracy': round(subject_accuracy, 2)
            }
            
            print(f"    üìä {subject} accuracy: {subject_accuracy:.2f}%")
        
        overall_accuracy = correct_answers / total_questions * 100 if total_questions > 0 else 0
        
        return {
            'task': 'MMLU Test',
            'overall_accuracy': round(overall_accuracy, 2),
            'total_correct': correct_answers,
            'total_questions': total_questions,
            'subject_results': subject_results
        }

    def calculate_perplexity(self, text: str) -> float:
        """T√≠nh perplexity cho m·ªôt ƒëo·∫°n text."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss.item()
            perplexity = torch.exp(torch.tensor(loss)).item()
        
        return perplexity

    def run_perplexity_test(self) -> Dict:
        """Ch·∫°y Perplexity test tr√™n nhi·ªÅu ƒëo·∫°n text kh√°c nhau."""
        print("  üìä Running Perplexity Test...")
        
        perplexities = []
        passage_results = []
        
        for i, passage in enumerate(PERPLEXITY_PASSAGES):
            print(f"    üìù Testing passage {i+1}/{len(PERPLEXITY_PASSAGES)}")
            
            start_time = time.time()
            perplexity = self.calculate_perplexity(passage)
            calc_time = time.time() - start_time
            
            perplexities.append(perplexity)
            passage_results.append({
                'passage_id': i + 1,
                'perplexity': round(perplexity, 4),
                'calculation_time': round(calc_time, 3),
                'text_preview': passage[:100] + "..." if len(passage) > 100 else passage
            })
            
            print(f"      Perplexity: {perplexity:.4f} (calculated in {calc_time:.3f}s)")
        
        avg_perplexity = np.mean(perplexities)
        std_perplexity = np.std(perplexities)
        
        return {
            'task': 'Perplexity Test',
            'average_perplexity': round(avg_perplexity, 4),
            'std_perplexity': round(std_perplexity, 4),
            'min_perplexity': round(min(perplexities), 4),
            'max_perplexity': round(max(perplexities), 4),
            'passage_results': passage_results
        }
        
    def _run_single_benchmark(self, task_name: str, prompts: List[str]) -> Dict:
        print(f"  üìä Benchmarking: {task_name}")
        results = []
        for i, p in enumerate(prompts):
            results.append(self.generate_response(p, f"{task_name} #{i+1}"))
            time.sleep(1) # Ngh·ªâ gi·ªØa c√°c l·∫ßn ch·∫°y ƒë·ªÉ ·ªïn ƒë·ªãnh
        
        avg_time = np.mean([r['generation_time'] for r in results])
        avg_tps = np.mean([r['tokens_per_second'] for r in results])
        
        return {
            'task': task_name,
            'avg_generation_time': round(avg_time, 2),
            'avg_tokens_per_second': round(avg_tps, 2),
            'tests': results
        }

    def run_full_benchmark(self) -> Dict:
        """Ch·∫°y to√†n b·ªô b·ªô benchmark tr√™n model g·ªëc."""
        print("\nüöÄ Starting Original Model Comprehensive Benchmark (NO QUANTIZATION)")
        print("=" * 80)
        print(f"üéØ Model Precision: {self.model_precision}")
        print(f"üíæ Device: {self.device}")
        
        # === MMLU v√† Perplexity Tests ===
        print("\nüß™ Running Academic & Technical Evaluations...")
        mmlu_results = self.run_mmlu_test()
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        perplexity_results = self.run_perplexity_test()
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # === Traditional Generation Tasks ===
        print("\nüìù Running Traditional Generation Tasks...")
        tasks = {
            "Code Generation": [
                "Vi·∫øt m·ªôt script Python s·ª≠ d·ª•ng th∆∞ vi·ªán Pandas ƒë·ªÉ ƒë·ªçc file CSV c√≥ t√™n `sales_data.csv` v·ªõi c√°c c·ªôt 'Date', 'Product', 'Revenue'. Script c·∫ßn t√≠nh t·ªïng doanh thu theo t·ª´ng s·∫£n ph·∫©m v√† xu·∫•t k·∫øt qu·∫£ ra m·ªôt file CSV m·ªõi c√≥ t√™n `revenue_by_product.csv`.",
                "T·∫°o m·ªôt component React functional b·∫±ng TypeScript t√™n l√† `UserProfile`. Component n√†y nh·∫≠n v√†o props l√† `name` (string), `age` (number), v√† `avatarUrl` (string), sau ƒë√≥ hi·ªÉn th·ªã th√¥ng tin n√†y m·ªôt c√°ch c√≥ c·∫•u tr√∫c."
            ],
            "Math Problem Solving": [
                "M·ªôt b·ªÉ n∆∞·ªõc c√≥ hai v√≤i. V√≤i th·ª© nh·∫•t ch·∫£y m·ªôt m√¨nh th√¨ ƒë·∫ßy b·ªÉ trong 4 gi·ªù. V√≤i th·ª© hai ch·∫£y m·ªôt m√¨nh th√¨ ƒë·∫ßy b·ªÉ trong 6 gi·ªù. N·∫øu m·ªü c·∫£ hai v√≤i c√πng m·ªôt l√∫c khi b·ªÉ c·∫°n, h·ªèi sau bao l√¢u th√¨ b·ªÉ s·∫Ω ƒë·∫ßy? Tr√¨nh b√†y c√°c b∆∞·ªõc gi·∫£i chi ti·∫øt.",
                "M·ªôt ng∆∞·ªùi g·ª≠i ti·∫øt ki·ªám 500 tri·ªáu ƒë·ªìng v·ªõi l√£i su·∫•t k√©p 6.5% m·ªói nƒÉm. H·ªèi sau 5 nƒÉm, ng∆∞·ªùi ƒë√≥ s·∫Ω nh·∫≠n ƒë∆∞·ª£c c·∫£ v·ªën l·∫´n l√£i l√† bao nhi√™u ti·ªÅn? Y√™u c·∫ßu tr√¨nh b√†y c√¥ng th·ª©c v√† c√°c b∆∞·ªõc t√≠nh to√°n."
            ],
            "Text Summarization": [
                "H√£y t√≥m t·∫Øt ƒëo·∫°n vƒÉn sau th√†nh 3 √Ω ch√≠nh: 'C√°c c√¥ng ngh·ªá thu gi·ªØ carbon (Carbon Capture Technologies) ƒëang n·ªïi l√™n nh∆∞ m·ªôt gi·∫£i ph√°p ti·ªÅm nƒÉng trong cu·ªôc chi·∫øn ch·ªëng bi·∫øn ƒë·ªïi kh√≠ h·∫≠u. C√°c ph∆∞∆°ng ph√°p ch√≠nh bao g·ªìm thu gi·ªØ sau ƒë·ªët ch√°y, thu gi·ªØ tr∆∞·ªõc ƒë·ªët ch√°y v√† thu gi·ªØ t·ª´ kh√¥ng kh√≠ tr·ª±c ti·∫øp (DAC). M·∫∑c d√π c√≥ ti·ªÅm nƒÉng l·ªõn, c√¥ng ngh·ªá n√†y v·∫´n ƒë·ªëi m·∫∑t v·ªõi nh·ªØng th√°ch th·ª©c v·ªÅ chi ph√≠ v·∫≠n h√†nh cao, hi·ªáu qu·∫£ nƒÉng l∆∞·ª£ng v√† v·∫•n ƒë·ªÅ l∆∞u tr·ªØ carbon an to√†n trong d√†i h·∫°n. C√°c ch√≠nh ph·ªß v√† t·∫≠p ƒëo√†n l·ªõn ƒëang ƒë·∫ßu t∆∞ h√†ng t·ª∑ USD v√†o R&D ƒë·ªÉ c·∫£i thi·ªán hi·ªáu qu·∫£ v√† gi·∫£m gi√° th√†nh, hy v·ªçng bi·∫øn n√≥ th√†nh m·ªôt c√¥ng c·ª• ch·ªß ch·ªët v√†o nƒÉm 2050.'",
                "T√≥m t·∫Øt ng·∫Øn g·ªçn nh·ªØng s·ª± ki·ªán ch√≠nh v√† √Ω nghƒ©a l·ªãch s·ª≠ c·ªßa cu·ªôc C√°ch m·∫°ng C√¥ng nghi·ªáp l·∫ßn th·ª© nh·∫•t, t·∫≠p trung v√†o c√°c ph√°t minh quan tr·ªçng v√† t√°c ƒë·ªông c·ªßa n√≥ ƒë·∫øn x√£ h·ªôi."
            ],
            "Creative Writing": [
                "Vi·∫øt ƒëo·∫°n m·ªü ƒë·∫ßu cho m·ªôt c√¢u chuy·ªán ng·∫Øn thu·ªôc th·ªÉ lo·∫°i khoa h·ªçc vi·ªÖn t∆∞·ªüng, trong ƒë√≥ nh√¢n v·∫≠t ch√≠nh l√† m·ªôt nh√† th·ª±c v·∫≠t h·ªçc s·ªëng tr√™n Sao H·ªèa, ng∆∞·ªùi v·ª´a ph√°t hi·ªán ra m·ªôt lo√†i c√¢y c√≥ kh·∫£ nƒÉng giao ti·∫øp b·∫±ng √°nh s√°ng.",
                "Vi·∫øt m·ªôt b√†i vƒÉn ng·∫Øn (kho·∫£ng 150 t·ª´) mi√™u t·∫£ c·∫£nh m·ªôt phi√™n ch·ª£ n·ªïi tr√™n s√¥ng C·ª≠u Long v√†o bu·ªïi s√°ng s·ªõm, t·∫≠p trung v√†o √¢m thanh, m√†u s·∫Øc v√† m√πi v·ªã ƒë·∫∑c tr∆∞ng."
            ],
            "Question Answering": [
                "Gi·∫£i th√≠ch s·ª± kh√°c bi·ªát c∆° b·∫£n gi·ªØa nƒÉng l∆∞·ª£ng h·∫°t nh√¢n ph√¢n h·∫°ch (nuclear fission) v√† t·ªïng h·ª£p h·∫°t nh√¢n (nuclear fusion). Lo·∫°i n√†o hi·ªán ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c nh√† m√°y ƒëi·ªán?",
                "Con ƒë∆∞·ªùng t∆° l·ª•a l√† g√¨ v√† n√≥ c√≥ vai tr√≤ quan tr·ªçng nh∆∞ th·∫ø n√†o ƒë·ªëi v·ªõi s·ª± ph√°t tri·ªÉn c·ªßa c√°c n·ªÅn vƒÉn minh c·ªï ƒë·∫°i?"
            ],
            "Language Translation & Nuance": [
                "D·ªãch ƒëo·∫°n vƒÉn sau sang ti·∫øng Vi·ªát, ch√∫ √Ω gi·ªØ vƒÉn phong chuy√™n nghi·ªáp: 'Our team is conducting a comprehensive due diligence process to assess the viability of the potential acquisition. Key performance indicators (KPIs) and financial statements are under rigorous scrutiny.'",
                "Gi·∫£i th√≠ch √Ω nghƒ©a v√† t√¨m c√¢u th√†nh ng·ªØ ti·∫øng Anh c√≥ nghƒ©a t∆∞∆°ng ƒë∆∞∆°ng v·ªõi c√¢u 'N∆∞·ªõc ƒë·∫øn ch√¢n m·ªõi nh·∫£y'."
            ],
            "Reasoning & Logic": [
                "C√≥ ba chi·∫øc h·ªôp. M·ªôt h·ªôp ch·ª©a to√†n bi ƒë·ªè, m·ªôt h·ªôp ch·ª©a to√†n bi xanh, v√† m·ªôt h·ªôp ch·ª©a l·∫´n l·ªôn c·∫£ bi ƒë·ªè v√† bi xanh. C·∫£ ba h·ªôp ƒë·ªÅu b·ªã d√°n nh√£n sai. B·∫°n ch·ªâ ƒë∆∞·ª£c ph√©p l·∫•y ra m·ªôt vi√™n bi t·ª´ m·ªôt h·ªôp duy nh·∫•t (kh√¥ng ƒë∆∞·ª£c nh√¨n v√†o b√™n trong). L√†m th·∫ø n√†o ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c n·ªôi dung c·ªßa c·∫£ ba h·ªôp? H√£y gi·∫£i th√≠ch qu√° tr√¨nh suy lu·∫≠n c·ªßa b·∫°n.",
                "M·ªôt nghi√™n c·ª©u cho th·∫•y nh·ªØng th√†nh ph·ªë c√≥ nhi·ªÅu c·ª≠a h√†ng kem nh·∫•t c≈©ng c√≥ t·ª∑ l·ªá t·ªôi ph·∫°m cao nh·∫•t. C√≥ ph·∫£i ƒÉn kem g√¢y ra t·ªôi ph·∫°m kh√¥ng? H√£y gi·∫£i th√≠ch v·ªÅ m·ªëi t∆∞∆°ng quan v√† quan h·ªá nh√¢n qu·∫£ (correlation vs. causation) trong tr∆∞·ªùng h·ª£p n√†y."
            ],
            "Technical Explanation": [
                "H√£y gi·∫£i th√≠ch cho m·ªôt ng∆∞·ªùi kh√¥ng r√†nh v·ªÅ c√¥ng ngh·ªá v·ªÅ kh√°i ni·ªám 'ƒêi·ªán to√°n ƒë√°m m√¢y' (Cloud Computing) l√† g√¨. S·ª≠ d·ª•ng v√≠ d·ª• v·ªÅ Google Drive ho·∫∑c iCloud ƒë·ªÉ minh h·ªça.",
                "Gi·∫£i th√≠ch m·ªôt c√°ch ƒë∆°n gi·∫£n qu√° tr√¨nh quang h·ª£p ·ªü th·ª±c v·∫≠t di·ªÖn ra nh∆∞ th·∫ø n√†o v√† t·∫°i sao n√≥ l·∫°i quan tr·ªçng ƒë·ªëi v·ªõi s·ª± s·ªëng tr√™n Tr√°i ƒê·∫•t."
            ]
        }

        benchmark_results = []
        for name, prompts in tasks.items():
            benchmark_results.append(self._run_single_benchmark(name, prompts))
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None

        # === T·ªïng h·ª£p k·∫øt qu·∫£ ===
        overall_tps = [r['avg_tokens_per_second'] for r in benchmark_results]
        memory_info = self.get_memory_usage()
        
        overall_stats = {
            'avg_token_speed': round(np.mean(overall_tps), 2) if overall_tps else 0,
            'mmlu_accuracy': mmlu_results['overall_accuracy'],
            'avg_perplexity': perplexity_results['average_perplexity'],
            'model_precision': self.model_precision,
            'peak_vram_allocated_gb': round(memory_info['gpu_allocated_gb'], 2),
            'peak_vram_reserved_gb': round(memory_info['gpu_reserved_gb'], 2)
        }
        
        return {
            'model_type': 'original_no_quantization',
            'model_hash': self.model_hash,
            'timestamp': datetime.now().isoformat(),
            'overall_stats': overall_stats,
            'mmlu_results': mmlu_results,
            'perplexity_results': perplexity_results,
            'benchmark_results': benchmark_results
        }

    def save_and_print_summary(self, results: Dict):
        """L∆∞u v√† in k·∫øt qu·∫£ benchmark."""
        stats = results['overall_stats']
        print("\n" + "="*80 + "\nüìä ORIGINAL MODEL BENCHMARK RESULTS SUMMARY\n" + "="*80)
        print(f"üéØ Model Type: Original (No Quantization)")
        print(f"üîë Model Hash: {results['model_hash']}")
        print(f"üé≠ Model Precision: {stats['model_precision']}")
        print(f"üìà Average Token Speed (Overall): {stats['avg_token_speed']:.2f} tokens/sec")
        print(f"üß† MMLU Accuracy: {stats['mmlu_accuracy']:.2f}%")
        print(f"üìä Average Perplexity: {stats['avg_perplexity']:.4f}")
        print(f"üíæ Peak VRAM Usage: {stats['peak_vram_allocated_gb']:.2f} GB (Reserved: {stats['peak_vram_reserved_gb']:.2f} GB)")
        
        print("\nüß™ Academic Test Results:")
        print(f"  üìö MMLU: {results['mmlu_results']['total_correct']}/{results['mmlu_results']['total_questions']} correct ({stats['mmlu_accuracy']:.2f}%)")
        
        mmlu_by_subject = results['mmlu_results']['subject_results']
        for subject, data in mmlu_by_subject.items():
            print(f"    - {subject:<20}: {data['accuracy']:>6.2f}%")
        
        print(f"  üìä Perplexity Range: {results['perplexity_results']['min_perplexity']:.4f} - {results['perplexity_results']['max_perplexity']:.4f}")
        
        print("\nüìã Generation Task Performance:")
        for result in results['benchmark_results']:
            vram = result['tests'][0]['vram_allocated_gb'] if result['tests'] else 'N/A'
            print(f"  - {result['task']:<25}: {result['avg_tokens_per_second']:.2f} tok/s @ {vram} GB VRAM")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"original_model_benchmark_{self.model_hash}_{timestamp}.json"
        filepath = Path("benchmark_results") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ Full detailed results saved to: {filepath}")

# === H√ÄM MAIN ƒê·ªÇ CH·∫†Y ORIGINAL MODEL BENCHMARK ===
def main():
    print("üîç Original Model Benchmark System (No EWQ Quantization)")
    print("=" * 80)
    
    try:
        model_hash = get_model_hash(MODEL_ID)
        print(f"üîë Model ID: {MODEL_ID}")
        print(f"üîë Model Hash: {model_hash}")
        
        model, tokenizer = load_original_model(MODEL_ID)
        
        if model is None or tokenizer is None:
            print("‚ùå Failed to load original model!")
            return
        
        print("‚úÖ Original model loaded and ready for benchmarking!")
        
        # Kh·ªüi t·∫°o benchmark suite
        benchmark_suite = OriginalModelBenchmarkSuite(model, tokenizer, model_hash)
        
        # Ch·∫°y to√†n b·ªô benchmark
        start_time = time.time()
        results = benchmark_suite.run_full_benchmark()
        total_time = time.time() - start_time
        
        # Th√™m th√¥ng tin th·ªùi gian t·ªïng th·ªÉ
        results['total_benchmark_time'] = round(total_time, 2)
        
        # L∆∞u v√† in k·∫øt qu·∫£
        benchmark_suite.save_and_print_summary(results)
        
        print(f"\n‚è±Ô∏è Total benchmark time: {total_time:.2f} seconds")
        print("\nüéâ Original model benchmark completed successfully!")
        
        # D·ªçn d·∫πp b·ªô nh·ªõ
        del model
        del tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Benchmark interrupted by user.")
        
    except Exception as e:
        print(f"\n‚ùå Error during benchmark: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # D·ªçn d·∫πp cu·ªëi c√πng
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("\nüßπ Memory cleanup completed.")


if __name__ == "__main__":
    main()