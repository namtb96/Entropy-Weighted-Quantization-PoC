from llama_cpp import Llama
from pathlib import Path
from typing import Optional

from .utils_gguf import download_gguf_model

def load_gguf_model(
    repo_id: str,
    model_file: str,
    model_cache_dir: str,
    n_gpu_layers: int = -1,
    n_ctx: int = 4096,
    verbose: bool = False
) -> Optional[Llama]:
    """
    Táº£i model GGUF sá»­ dá»¥ng llama-cpp-python.

    Args:
        repo_id (str): ID cá»§a repository trÃªn Hugging Face (vÃ­ dá»¥: 'Qwen/Qwen2-7B-Instruct-GGUF').
        model_file (str): TÃªn file .gguf cá»¥ thá»ƒ (vÃ­ dá»¥: 'qwen2-7b-instruct-q4_k_m.gguf').
        model_cache_dir (str): ThÆ° má»¥c Ä‘á»ƒ lÆ°u trá»¯ model.
        n_gpu_layers (int): Sá»‘ lÆ°á»£ng layer Ä‘á»ƒ offload lÃªn GPU. -1 Ä‘á»ƒ offload táº¥t cáº£.
        n_ctx (int): KÃ­ch thÆ°á»›c ngá»¯ cáº£nh tá»‘i Ä‘a cá»§a model.
        verbose (bool): Báº­t/táº¯t log chi tiáº¿t tá»« llama.cpp.

    Returns:
        Má»™t Ä‘á»‘i tÆ°á»£ng Llama hoáº·c None náº¿u cÃ³ lá»—i.
    """
    try:
        model_path = download_gguf_model(repo_id, model_file, model_cache_dir)
        
        print(f"  ðŸš€ Loading GGUF model into memory...")
        print(f"     - Model Path: {model_path}")
        print(f"     - GPU Layers: {'All' if n_gpu_layers == -1 else n_gpu_layers}")
        print(f"     - Context Size: {n_ctx}")

        llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=n_gpu_layers,
            n_ctx=n_ctx,
            verbose=verbose,
            chat_format="chatml", # Qwen sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng ChatML
            logits_all=True  # <<< THAY Äá»”I QUAN TRá»ŒNG: Báº¯t buá»™c Ä‘á»ƒ tÃ­nh perplexity
        )
        print("  âœ… GGUF Model loaded successfully!")
        return llm
    except Exception as e:
        print(f"  âŒ Failed to load GGUF model: {e}")
        return None