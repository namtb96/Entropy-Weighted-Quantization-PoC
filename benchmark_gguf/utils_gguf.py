import json
import hashlib
from pathlib import Path
from huggingface_hub import hf_hub_download

def get_model_hash(model_id: str, config: dict) -> str:
    """Táº¡o má»™t hash duy nháº¥t cho má»™t cáº¥u hÃ¬nh model GGUF cá»¥ thá»ƒ."""
    config_str = f"{model_id}-{json.dumps(config, sort_keys=True)}"
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]

def create_qwen_prompt(user_prompt):
    """Táº¡o prompt theo Ä‘á»‹nh dáº¡ng chat cá»§a Qwen. Giá»¯ nguyÃªn vÃ¬ Ä‘Ã¢y lÃ  Ä‘áº·c trÆ°ng cá»§a model."""
    system_prompt = "You are a helpful assistant. Provide a direct and concise answer to the user's request. Do not show your thought process, reasoning steps, or self-correction. Respond only with the final answer."
    return f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_prompt}<|im_end|>\n<|im_start|>assistant\n"

def download_gguf_model(repo_id: str, filename: str, cache_dir: str) -> Path:
    """
    Táº£i file model GGUF tá»« Hugging Face Hub náº¿u chÆ°a cÃ³ vÃ  tráº£ vá» Ä‘Æ°á»ng dáº«n.
    """
    print(f"  ğŸ“¥ Checking for GGUF model file: {filename}")
    model_path = Path(cache_dir) / filename
    if not model_path.exists():
        print(f"  â¬ Model not found locally. Downloading from '{repo_id}'...")
        try:
            downloaded_path = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                local_dir=cache_dir,
                local_dir_use_symlinks=False # NÃªn Ä‘áº·t lÃ  False Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» trÃªn Windows
            )
            # hf_hub_download cÃ³ thá»ƒ Ä‘áº·t file vÃ o má»™t thÆ° má»¥c con snapshots, cáº§n tÃ¬m Ä‘Ãºng Ä‘Æ°á»ng dáº«n
            # Tuy nhiÃªn, vá»›i cÃ¡ch dÃ¹ng nÃ y, nÃ³ sáº½ lÆ°u trá»±c tiáº¿p vÃ o cache_dir
            print(f"  âœ… Download complete. Model saved at: {downloaded_path}")
            return Path(downloaded_path)

        except Exception as e:
            print(f"  âŒ Failed to download model: {e}")
            raise
    else:
        print("  ğŸ‘ Model found in cache.")
    return model_path