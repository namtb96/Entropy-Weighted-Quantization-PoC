import numpy as np
import csv
from pathlib import Path
from typing import Dict, Optional, List
from tqdm import tqdm
from llama_cpp import Llama
from datasets import load_dataset
from evaluate import load 

def _load_mmlu_data_from_csv(mmlu_dir: Path, max_questions_per_subject: Optional[int] = None) -> Dict:
    print(f"  ðŸ” Searching for MMLU test files in: {mmlu_dir}")
    if not mmlu_dir.is_dir():
        print(f"  âš ï¸ Warning: MMLU directory not found at '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    all_questions = {}
    csv_files = sorted(list(mmlu_dir.glob("*_test.csv")))
    if not csv_files:
        print(f"  âš ï¸ Warning: No '*_test.csv' files found in '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    for csv_file in csv_files:
        subject_name = csv_file.name.replace("_test.csv", "")
        subject_questions = []
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) != 6: continue
                    question_text, choice_a, choice_b, choice_c, choice_d, answer_letter = row
                    answer_index = ord(answer_letter.strip().upper()) - ord('A')
                    q_data = {"question": question_text, "choices": [choice_a, choice_b, choice_c, choice_d], "answer": answer_index}
                    subject_questions.append(q_data)
            if max_questions_per_subject:
                all_questions[subject_name] = subject_questions[:max_questions_per_subject]
            else:
                all_questions[subject_name] = subject_questions
        except Exception as e:
            print(f"    âŒ Error reading file {csv_file.name}: {e}")
    return all_questions

def run_mmlu_test(llm: Llama, mmlu_dir: Path) -> Dict:
    print("  ðŸ§  Running MMLU Test for GGUF model...")
    mmlu_data = _load_mmlu_data_from_csv(mmlu_dir)
    if not mmlu_data:
        return {'task': 'MMLU Test', 'status': 'Skipped', 'reason': 'No MMLU data found or loaded.'}
    
    correct_answers, total_questions, subject_results = 0, 0, {}
    
    for subject, questions in mmlu_data.items():
        print(f"    ðŸ“š Testing subject: {subject} ({len(questions)} questions)")
        subject_correct = 0
        for q in tqdm(questions, desc=f"  -> {subject}", leave=False):
            choices_text = "\n".join([f"{chr(65+j)}. {choice}" for j, choice in enumerate(q['choices'])])
            # Prompt Ä‘Æ¡n giáº£n Ä‘á»ƒ model chá»‰ tráº£ vá» kÃ½ tá»± A, B, C, hoáº·c D
            prompt = f"Question: {q['question']}\n{choices_text}\nAnswer:"
            
            completion = llm.create_completion(
                prompt,
                max_tokens=5, # Chá»‰ cáº§n má»™t token lÃ  Ä‘á»§
                temperature=0.0 # Láº¥y cÃ¢u tráº£ lá»i cháº¯c cháº¯n nháº¥t
            )
            response = completion['choices'][0]['text'].strip()
            
            predicted_letter = response[0].upper() if response and response[0].upper() in 'ABCD' else 'X'
            correct_letter = chr(65 + q['answer'])
            if predicted_letter == correct_letter:
                subject_correct += 1
        
        correct_answers += subject_correct
        total_questions += len(questions)
        subject_accuracy = subject_correct / len(questions) * 100 if questions else 0
        subject_results[subject] = {'correct': subject_correct, 'total': len(questions), 'accuracy': round(subject_accuracy, 2)}
        print(f"    ðŸ“Š {subject} accuracy: {subject_accuracy:.2f}% ({subject_correct}/{len(questions)})")
    
    overall_accuracy = correct_answers / total_questions * 100 if total_questions > 0 else 0
    return {'task': 'MMLU Test', 'overall_accuracy': round(overall_accuracy, 2), 'total_correct': correct_answers, 'total_questions': total_questions}

def _load_generation_test_data(dataset_name: str, split: str, prompt_column: str, reference_column: str, num_samples: int, cache_dir: str) -> List[Dict[str, str]]:
    """
    Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u tá»« má»™t dataset trÃªn Hugging Face Hub, sá»­ dá»¥ng cache.
    """
    print(f"\n  ðŸ“¦ Loading {num_samples} samples from Hugging Face dataset '{dataset_name}'...")
    print(f"  ðŸ’¾ Cache directory: '{Path(cache_dir).resolve()}'")
    try:
        dataset = load_dataset(dataset_name, split=f"{split}[:{num_samples}]", cache_dir=cache_dir)
    except Exception as e:
        print(f"  âŒ Failed to load dataset '{dataset_name}'. Error: {e}")
        return []

    formatted_data = []
    for sample in dataset:
        if prompt_column not in sample or reference_column not in sample:
            continue
        prompt = f"Summarize the following text:\n\n{sample[prompt_column]}"
        reference = sample[reference_column]
        formatted_data.append({'prompt': prompt, 'reference': reference})
    
    return formatted_data

def run_bleu_rouge_test(llm: Llama, dataset_name: str, split: str, prompt_column: str, reference_column: str, num_samples: int = 50) -> Optional[Dict]:
    """
    Cháº¡y kiá»ƒm tra cháº¥t lÆ°á»£ng sinh vÄƒn báº£n báº±ng cÃ¡ch táº£i dá»¯ liá»‡u, táº¡o dá»± Ä‘oÃ¡n vÃ  tÃ­nh Ä‘iá»ƒm BLEU & ROUGE.
    Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c cache vÃ o thÆ° má»¥c './generation_test_cache'.
    """
    print("\n  âœï¸  Running BLEU & ROUGE Generation Quality Test...")
    
    # Äá»‹nh nghÄ©a thÆ° má»¥c cache
    cache_directory = "./generation_test_cache"

    # Táº£i dá»¯ liá»‡u bÃªn trong hÃ m
    test_data = _load_generation_test_data(
        dataset_name=dataset_name,
        split=split,
        prompt_column=prompt_column,
        reference_column=reference_column,
        num_samples=num_samples,
        cache_dir=cache_directory
    )

    if not test_data:
        print("  âš ï¸ Skipping test as no data was loaded.")
        return {'task': 'BLEU/ROUGE Test', 'status': 'Skipped', 'reason': 'Failed to load test data.'}

    try:
        bleu_metric = load('bleu')
        rouge_metric = load('rouge')
    except Exception as e:
        print(f"  âŒ Error loading metrics: {e}")
        return {'task': 'BLEU/ROUGE Test', 'status': 'Failed', 'reason': str(e)}

    predictions = []
    references = []

    print(f"  ðŸš€ Generating {len(test_data)} predictions from the model...")
    for item in tqdm(test_data, desc="  -> Generating", leave=False):
        prompt = item['prompt']
        reference_text = item['reference']
        
        completion = llm.create_completion(prompt, max_tokens=256, temperature=0.1, stop=["\n\n", "Summarize the following text:"])
        prediction_text = completion['choices'][0]['text'].strip()
        
        predictions.append(prediction_text)
        references.append(reference_text)

    print("  ðŸ”¢ Calculating scores...")
    bleu_score = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])
    rouge_score = rouge_metric.compute(predictions=predictions, references=references)

    final_results = {
        'bleu': round(bleu_score['bleu'], 4),
        'rouge1': round(rouge_score['rouge1'], 4),
        'rouge2': round(rouge_score['rouge2'], 4),
        'rougeL': round(rouge_score['rougeL'], 4),
        'dataset': dataset_name,
        'num_samples': len(predictions)
    }
    
    print(f"    ðŸ“Š BLEU Score: {final_results['bleu']:.4f}")
    print(f"    ðŸ“Š ROUGE-L Score: {final_results['rougeL']:.4f}")
    
    return {'task': 'BLEU/ROUGE Test', 'results': final_results}