import psutil
import time
import numpy as np
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from llama_cpp import Llama
import os
import subprocess

from . import tasks_gguf as tasks

class NpEncoder(json.JSONEncoder):
    """
    JSON encoder t√πy ch·ªânh ƒë·ªÉ x·ª≠ l√Ω c√°c ki·ªÉu d·ªØ li·ªáu c·ªßa NumPy.
    """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NpEncoder, self).default(obj)

def _get_nvidia_vram_usage() -> float:
    """
    L·∫•y m·ª©c s·ª≠ d·ª•ng VRAM c·ªßa ti·∫øn tr√¨nh hi·ªán t·∫°i b·∫±ng nvidia-smi.
    Ch·ªâ ho·∫°t ƒë·ªông tr√™n GPU NVIDIA.

    Returns:
        M·ª©c s·ª≠ d·ª•ng VRAM t√≠nh b·∫±ng GB. Tr·∫£ v·ªÅ 0.0 n·∫øu kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c.
    """
    try:
        # L·∫•y PID c·ªßa ti·∫øn tr√¨nh hi·ªán t·∫°i
        pid = os.getpid()
        # T·∫°o c√¢u l·ªánh nvidia-smi ƒë·ªÉ truy v·∫•n b·ªô nh·ªõ GPU ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi PID c·ª• th·ªÉ
        cmd = [
            'nvidia-smi',
            '--query-compute-apps=pid,used_gpu_memory',
            '--format=csv,noheader,nounits'
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # Ph√¢n t√≠ch output
        lines = result.stdout.strip().split('\n')
        for line in lines:
            parts = line.split(', ')
            if len(parts) == 2 and int(parts[0]) == pid:
                # Gi√° tr·ªã tr·∫£ v·ªÅ l√† MiB, chuy·ªÉn ƒë·ªïi sang GB
                vram_mib = int(parts[1])
                return vram_mib / 1024
        return 0.0 # Tr·∫£ v·ªÅ 0 n·∫øu kh√¥ng t√¨m th·∫•y PID trong output
    except (FileNotFoundError, subprocess.CalledProcessError, Exception):
        return 0.0


class EnhancedBenchmarkSuiteGGUF:
    def __init__(self, llm: Llama, model_id: str, run_name: str, model_hash: str):
        self.llm = llm
        self.model_id = model_id
        self.run_name = run_name
        self.model_hash = model_hash
        self.stop_tokens = ["<|im_end|>", "<|endoftext|>"]
        # Ki·ªÉm tra m·ªôt l·∫ßn xem c√≥ th·ªÉ l·∫•y th√¥ng tin VRAM kh√¥ng
        self.vram_gb_at_start = _get_nvidia_vram_usage()
        if self.vram_gb_at_start > 0:
            print(f"  ‚úÖ Detected NVIDIA GPU. Initial VRAM usage: {self.vram_gb_at_start:.2f} GB")
        else:
            print("  ‚ö†Ô∏è Could not detect NVIDIA VRAM usage via nvidia-smi. VRAM will be reported as 0.")


    def _get_memory_usage(self) -> Dict[str, float]:
        process = psutil.Process()
        ram_gb = process.memory_info().rss / 1024**3
        # S·ª≠ d·ª•ng gi√° tr·ªã ƒë√£ ki·ªÉm tra l√∫c kh·ªüi t·∫°o
        gpu_allocated_gb = self.vram_gb_at_start 
        return {'ram_gb': ram_gb, 'gpu_allocated_gb': gpu_allocated_gb}

    def _generate_response(self, prompt: str, task_name: str = "") -> Dict:
        print(f"    üîÑ Generating response for: {task_name}")
        start_time = time.time()
        completion = self.llm.create_completion(
            prompt, max_tokens=4096, temperature=0.6, top_p=0.9,
            stop=self.stop_tokens, echo=False
        )
        end_time = time.time()
        
        response_text = completion['choices'][0]['text'].strip()
        usage_info = completion['usage']
        
        generation_time = end_time - start_time
        tokens_generated = usage_info['completion_tokens']
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        # L·∫•y th√¥ng tin VRAM (s·∫Ω l√† gi√° tr·ªã tƒ©nh ƒë∆∞·ª£c ƒëo khi t·∫£i model)
        vram_usage = self._get_memory_usage()['gpu_allocated_gb']

        return {
            'prompt': prompt,
            'response': response_text,
            'generation_time': round(generation_time, 2),
            'tokens_generated': tokens_generated,
            'tokens_per_second': round(tokens_per_second, 2),
            'vram_usage_gb': round(vram_usage, 2) # Tr·∫£ v·ªÅ gi√° tr·ªã VRAM ƒë√£ ƒëo
        }

    def _run_single_generation_task(self, task_name: str, prompts: List[str]) -> Dict:
        print(f"  üìä Benchmarking Generation Task: {task_name}")
        results = [self._generate_response(p, f"{task_name} #{i+1}") for i, p in enumerate(prompts)]
        avg_tps = np.mean([r['tokens_per_second'] for r in results])
        self.llm.reset()
        return {'task': task_name, 'avg_tokens_per_second': avg_tps, 'tests': results} # Gi·ªØ np.float ƒë·ªÉ encoder x·ª≠ l√Ω

    def run_full_benchmark(self, mmlu_dir: Path, PERPLEXITY_CATEGORIES: list, traditional_tasks: dict):
        print("\nüöÄ Starting GGUF Model Comprehensive Benchmark")
        print("=" * 80)

        mmlu_results = tasks.run_mmlu_test(self.llm, mmlu_dir)
        perplexity_results = tasks.run_perplexity_test(self.llm, PERPLEXITY_CATEGORIES)
        
        generation_results = []
        for name, prompts in traditional_tasks.items():
            generation_results.append(self._run_single_generation_task(name, prompts))

        overall_stats = {
            'avg_token_speed': np.mean([r['avg_tokens_per_second'] for r in generation_results]) if generation_results else 0,
            'mmlu_accuracy': mmlu_results.get('overall_accuracy', 0),
            'avg_perplexity': perplexity_results.get('overall_average_perplexity', 0),
            'vram_usage_gb': self.vram_gb_at_start # Th√™m th√¥ng s·ªë n√†y v√†o k·∫øt qu·∫£ t·ªïng th·ªÉ
        }
        
        full_results = {
            'model_hash': self.model_hash,
            'timestamp': datetime.now().isoformat(),
            'overall_stats': overall_stats,
            'mmlu_results': mmlu_results,
            'perplexity_results': perplexity_results,
            'generation_results': generation_results
        }
        self.save_and_print_summary(full_results)
        return full_results
    
    def save_and_print_summary(self, results: Dict):
        """L∆∞u v√† in k·∫øt qu·∫£ benchmark v·ªõi t√™n file d·ªÖ ƒë·ªçc."""
        stats = results['overall_stats']
        
        print("\n" + "="*80 + "\nüìä GGUF MODEL ENHANCED BENCHMARK RESULTS SUMMARY\n" + "="*80)
        print(f"üì¶ Model ID: {self.model_id}")
        print(f"‚öôÔ∏è Run Type: {self.run_name}")
        print(f"üéØ Model Hash: {results['model_hash']}")
        print("-" * 80)
        # Th√™m th√¥ng tin VRAM v√†o b·∫£n t√≥m t·∫Øt
        if stats.get('vram_usage_gb', 0) > 0:
            print(f"üíæ VRAM Usage: {float(stats['vram_usage_gb']):.2f} GB")
        print(f"üìà Average Token Speed (Generation): {float(stats['avg_token_speed']):.2f} tokens/sec")
        print(f"üß† MMLU Accuracy: {float(stats['mmlu_accuracy']):.2f}%")
        print(f"üìä Overall Average Perplexity: {float(stats['avg_perplexity']):.4f}")
        
        if 'category_results' in results['perplexity_results']:
            print("\n   --- Perplexity by Category ---")
            for category, data in results['perplexity_results']['category_results'].items():
                print(f"   - {category:<25}: {float(data['average_perplexity']):.4f}")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_model_name = self.model_id.replace('/', '_')
        filename = f"benchmark_gguf_{safe_model_name}_{self.run_name}_{timestamp}.json"
        filepath = Path("benchmark_results") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            results['run_name'] = self.run_name
            json.dump(results, f, indent=2, ensure_ascii=False, cls=NpEncoder)
            
        print(f"\nüíæ Full detailed results saved to: {filepath}")