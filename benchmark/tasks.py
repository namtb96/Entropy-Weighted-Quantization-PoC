import torch
import numpy as np
import csv
from pathlib import Path
from typing import Dict, Optional, List
from tqdm import tqdm
from evaluate import load
from datasets import load_dataset

def _load_mmlu_data_from_csv(mmlu_dir: Path, max_questions_per_subject: Optional[int] = None) -> Dict:
    print(f"  ğŸ” Searching for MMLU test files in: {mmlu_dir}")
    if not mmlu_dir.is_dir():
        print(f"  âš ï¸ Warning: MMLU directory not found at '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    all_questions = {}
    csv_files = sorted(list(mmlu_dir.glob("*_test.csv")))
    if not csv_files:
        print(f"  âš ï¸ Warning: No '*_test.csv' files found in '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    for csv_file in csv_files:
        subject_name = csv_file.name.replace("_test.csv", "")
        subject_questions = []
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) != 6: continue
                    question_text, choice_a, choice_b, choice_c, choice_d, answer_letter = row
                    answer_index = ord(answer_letter.strip().upper()) - ord('A')
                    q_data = {"question": question_text, "choices": [choice_a, choice_b, choice_c, choice_d], "answer": answer_index}
                    subject_questions.append(q_data)
            if max_questions_per_subject:
                all_questions[subject_name] = subject_questions[:max_questions_per_subject]
            else:
                all_questions[subject_name] = subject_questions
        except Exception as e:
            print(f"    âŒ Error reading file {csv_file.name}: {e}")
    return all_questions

def run_mmlu_test(model, tokenizer, device, mmlu_dir: Path) -> Dict:
    print("  ğŸ§  Running MMLU Test from CSV files...")
    mmlu_data = _load_mmlu_data_from_csv(mmlu_dir)
    # ... (toÃ n bá»™ logic cá»§a run_mmlu_test giá»‘ng há»‡t phiÃªn báº£n trÆ°á»›c, nhÆ°ng nháº­n model, tokenizer, device lÃ m tham sá»‘) ...
    if not mmlu_data:
        return {'task': 'MMLU Test', 'status': 'Skipped', 'reason': 'No MMLU data found or loaded.'}
    
    correct_answers, total_questions, subject_results = 0, 0, {}
    
    for subject, questions in mmlu_data.items():
        print(f"    ğŸ“š Testing subject: {subject} ({len(questions)} questions)")
        subject_correct = 0
        for q in tqdm(questions, desc=f"  -> {subject}", leave=False):
            choices_text = "\n".join([f"{chr(65+j)}. {choice}" for j, choice in enumerate(q['choices'])])
            prompt = f"Question: {q['question']}\n{choices_text}\nAnswer:"
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False, temperature=0.1)
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
            predicted_letter = response[0].upper() if response and response[0].upper() in 'ABCD' else 'X'
            correct_letter = chr(65 + q['answer'])
            if predicted_letter == correct_letter:
                subject_correct += 1
        
        correct_answers += subject_correct
        total_questions += len(questions)
        subject_accuracy = subject_correct / len(questions) * 100 if questions else 0
        subject_results[subject] = {'correct': subject_correct, 'total': len(questions), 'accuracy': round(subject_accuracy, 2)}
        print(f"    ğŸ“Š {subject} accuracy: {subject_accuracy:.2f}% ({subject_correct}/{len(questions)})")
    
    overall_accuracy = correct_answers / total_questions * 100 if total_questions > 0 else 0
    return {'task': 'MMLU Test', 'overall_accuracy': round(overall_accuracy, 2), 'total_correct': correct_answers, 'total_questions': total_questions}

def _load_generation_test_data(dataset_name: str, split: str, prompt_column: str, reference_column: str, num_samples: int, cache_dir: str) -> List[Dict[str, str]]:
    """
    Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u tá»« má»™t dataset trÃªn Hugging Face Hub, sá»­ dá»¥ng cache.
    """
    print(f"\n  ğŸ“¦ Loading {num_samples} samples from Hugging Face dataset '{dataset_name}'...")
    print(f"  ğŸ’¾ Using cache directory: '{Path(cache_dir).resolve()}'")
    try:
        # Sá»­ dá»¥ng slicing Ä‘á»ƒ chá»‰ táº£i sá»‘ lÆ°á»£ng máº«u cáº§n thiáº¿t, ráº¥t hiá»‡u quáº£
        dataset = load_dataset(dataset_name, split=f"{split}[:{num_samples}]", cache_dir=cache_dir)
    except Exception as e:
        print(f"  âŒ Failed to load dataset '{dataset_name}'. Error: {e}")
        print("  ğŸ‘‰ Please check the dataset name, split, and your internet connection.")
        return []

    formatted_data = []
    for sample in dataset:
        if prompt_column not in sample or reference_column not in sample:
            continue
        # Táº¡o prompt rÃµ rÃ ng cho model
        prompt = f"Summarize the following text:\n\n{sample[prompt_column]}"
        reference = sample[reference_column]
        formatted_data.append({'prompt': prompt, 'reference': reference})
    
    return formatted_data

def run_bleu_rouge_test(model, tokenizer, device, dataset_name: str, split: str, prompt_column: str, reference_column: str, num_samples: int = 50) -> Optional[Dict]:
    """
    Cháº¡y kiá»ƒm tra cháº¥t lÆ°á»£ng sinh vÄƒn báº£n báº±ng cÃ¡ch táº£i dá»¯ liá»‡u, táº¡o dá»± Ä‘oÃ¡n vÃ  tÃ­nh Ä‘iá»ƒm BLEU & ROUGE.
    PhiÃªn báº£n nÃ y Ä‘Ã£ sá»­a lá»—i trÃ­ch xuáº¥t prediction Ä‘á»ƒ xá»­ lÃ½ trÆ°á»ng há»£p model khÃ´ng sinh token má»›i.
    """
    print("\n  âœï¸  Running BLEU & ROUGE Generation Quality Test...")
    
    cache_directory = "./generation_test_cache"
    test_data = _load_generation_test_data(
        dataset_name=dataset_name,
        split=split,
        prompt_column=prompt_column,
        reference_column=reference_column,
        num_samples=num_samples,
        cache_dir=cache_directory
    )

    if not test_data:
        print("  âš ï¸ Skipping test as no data was loaded.")
        return {'task': 'BLEU/ROUGE Test', 'status': 'Skipped', 'reason': 'Failed to load test data.'}

    try:
        bleu_metric = load('bleu')
        rouge_metric = load('rouge')
    except Exception as e:
        print(f"  âŒ Error loading metrics: {e}")
        return {'task': 'BLEU/ROUGE Test', 'status': 'Failed', 'reason': str(e)}

    predictions = []
    references = []

    print(f"  ğŸš€ Generating {len(test_data)} predictions from the model...")

    for item in tqdm(test_data, desc="  -> Generating", leave=False):
        prompt = item['prompt']
        # ThÃªm reference vÃ o danh sÃ¡ch ngay tá»« Ä‘áº§u Ä‘á»ƒ Ä‘áº£m báº£o nÃ³ luÃ´n khá»›p
        references.append(item['reference'])

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=8192).to(device)
        input_token_count = inputs.input_ids.shape[1] # Láº¥y sá»‘ lÆ°á»£ng token cá»§a prompt

        with torch.no_grad():
            output_tokens = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.1,
                do_sample=True,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Láº¥y chá»‰ cÃ¡c token Má»šI Ä‘Æ°á»£c sinh ra (bá» qua pháº§n prompt ban Ä‘áº§u)
        new_tokens = output_tokens[0, input_token_count:]
        
        # Giáº£i mÃ£ chá»‰ cÃ¡c token má»›i Ä‘Ã³
        prediction = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        
        # ThÃªm má»™t bÆ°á»›c kiá»ƒm tra Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng thÃªm chuá»—i rá»—ng
        if not prediction:
            print(f"  [Warning] Model generated an empty response for a prompt. Appending a placeholder.")
            # ThÃªm má»™t placeholder thay vÃ¬ chuá»—i rá»—ng Ä‘á»ƒ trÃ¡nh lá»—i
            # Hoáº·c báº¡n cÃ³ thá»ƒ xÃ³a reference tÆ°Æ¡ng á»©ng: references.pop()
            predictions.append("[empty response]") 
        else:
            predictions.append(prediction)
        # ===============================================================

    # Äáº£m báº£o sá»‘ lÆ°á»£ng predictions vÃ  references khá»›p nhau
    if len(predictions) != len(references):
        print("  [Error] Mismatch between number of predictions and references after generation. Aborting score calculation.")
        return {'task': 'BLEU/ROUGE Test', 'status': 'Failed', 'reason': 'Prediction/Reference count mismatch.'}

    print("  ğŸ”¢ Calculating scores...")
    # BLEU yÃªu cáº§u reference lÃ  list cá»§a list
    bleu_score = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])
    rouge_score = rouge_metric.compute(predictions=predictions, references=references)

    final_results = {
        'bleu': round(bleu_score['bleu'], 4),
        'rouge1': round(rouge_score['rouge1'], 4),
        'rouge2': round(rouge_score['rouge2'], 4),
        'rougeL': round(rouge_score['rougeL'], 4),
        'num_samples': len(predictions)
    }
    
    print(f"    ğŸ“Š BLEU Score: {final_results['bleu']:.4f}")
    print(f"    ğŸ“Š ROUGE-L Score: {final_results['rougeL']:.4f}")
    print("âœ… Test finished successfully.")

    return {
        'task': 'BLEU/ROUGE Test',
        'status': 'Success',
        'results': final_results
    }