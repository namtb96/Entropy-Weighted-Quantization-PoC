import torch
import numpy as np
import csv
from pathlib import Path
from typing import Dict, Optional
from tqdm import tqdm

def _load_mmlu_data_from_csv(mmlu_dir: Path, max_questions_per_subject: Optional[int] = None) -> Dict:
    print(f"  ğŸ” Searching for MMLU test files in: {mmlu_dir}")
    if not mmlu_dir.is_dir():
        print(f"  âš ï¸ Warning: MMLU directory not found at '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    all_questions = {}
    csv_files = sorted(list(mmlu_dir.glob("*_test.csv")))
    if not csv_files:
        print(f"  âš ï¸ Warning: No '*_test.csv' files found in '{mmlu_dir}'. Skipping MMLU test.")
        return {}
    for csv_file in csv_files:
        subject_name = csv_file.name.replace("_test.csv", "")
        subject_questions = []
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) != 6: continue
                    question_text, choice_a, choice_b, choice_c, choice_d, answer_letter = row
                    answer_index = ord(answer_letter.strip().upper()) - ord('A')
                    q_data = {"question": question_text, "choices": [choice_a, choice_b, choice_c, choice_d], "answer": answer_index}
                    subject_questions.append(q_data)
            if max_questions_per_subject:
                all_questions[subject_name] = subject_questions[:max_questions_per_subject]
            else:
                all_questions[subject_name] = subject_questions
        except Exception as e:
            print(f"    âŒ Error reading file {csv_file.name}: {e}")
    return all_questions

def run_mmlu_test(model, tokenizer, device, mmlu_dir: Path) -> Dict:
    print("  ğŸ§  Running MMLU Test from CSV files...")
    mmlu_data = _load_mmlu_data_from_csv(mmlu_dir)
    # ... (toÃ n bá»™ logic cá»§a run_mmlu_test giá»‘ng há»‡t phiÃªn báº£n trÆ°á»›c, nhÆ°ng nháº­n model, tokenizer, device lÃ m tham sá»‘) ...
    if not mmlu_data:
        return {'task': 'MMLU Test', 'status': 'Skipped', 'reason': 'No MMLU data found or loaded.'}
    
    correct_answers, total_questions, subject_results = 0, 0, {}
    
    for subject, questions in mmlu_data.items():
        print(f"    ğŸ“š Testing subject: {subject} ({len(questions)} questions)")
        subject_correct = 0
        for q in tqdm(questions, desc=f"  -> {subject}", leave=False):
            choices_text = "\n".join([f"{chr(65+j)}. {choice}" for j, choice in enumerate(q['choices'])])
            prompt = f"Question: {q['question']}\n{choices_text}\nAnswer:"
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False, temperature=0.1)
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
            predicted_letter = response[0].upper() if response and response[0].upper() in 'ABCD' else 'X'
            correct_letter = chr(65 + q['answer'])
            if predicted_letter == correct_letter:
                subject_correct += 1
        
        correct_answers += subject_correct
        total_questions += len(questions)
        subject_accuracy = subject_correct / len(questions) * 100 if questions else 0
        subject_results[subject] = {'correct': subject_correct, 'total': len(questions), 'accuracy': round(subject_accuracy, 2)}
        print(f"    ğŸ“Š {subject} accuracy: {subject_accuracy:.2f}% ({subject_correct}/{len(questions)})")
    
    overall_accuracy = correct_answers / total_questions * 100 if total_questions > 0 else 0
    return {'task': 'MMLU Test', 'overall_accuracy': round(overall_accuracy, 2), 'total_correct': correct_answers, 'total_questions': total_questions, 'subject_results': subject_results}

def run_perplexity_test(model, tokenizer, device, categories: Dict[str, list]) -> Dict:
    """
    Cháº¡y Perplexity test trÃªn cÃ¡c danh má»¥c vÄƒn báº£n khÃ¡c nhau.
    
    Args:
        model: Model ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c táº£i.
        tokenizer: Tokenizer tÆ°Æ¡ng á»©ng vá»›i model.
        device: Thiáº¿t bá»‹ Ä‘á»ƒ cháº¡y tÃ­nh toÃ¡n (vÃ­ dá»¥: 'cuda' hoáº·c 'cpu').
        categories: Má»™t tá»« Ä‘iá»ƒn trong Ä‘Ã³ key lÃ  tÃªn danh má»¥c (str) vÃ  
                    value lÃ  má»™t danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn (list[str]).

    Returns:
        Má»™t tá»« Ä‘iá»ƒn chá»©a káº¿t quáº£ chi tiáº¿t theo tá»«ng danh má»¥c vÃ  káº¿t quáº£ tá»•ng thá»ƒ.
    """
    print("  ğŸ“Š Running Perplexity Test across multiple categories...")
    
    category_results = {}
    all_perplexities = []
    
    # VÃ²ng láº·p ngoÃ i: láº·p qua tá»«ng danh má»¥c (vÃ­ dá»¥: 'general_knowledge', 'code_python')
    for category_name, passages in categories.items():
        print(f"    -> Testing category: '{category_name}' ({len(passages)} passages)")
        
        perplexities_for_this_category = []
        
        # VÃ²ng láº·p trong: láº·p qua tá»«ng Ä‘oáº¡n vÄƒn trong danh má»¥c hiá»‡n táº¡i
        for passage in tqdm(passages, desc=f"      {category_name}", leave=False):
            # Bá» qua cÃ¡c Ä‘oáº¡n vÄƒn trá»‘ng Ä‘á»ƒ trÃ¡nh lá»—i
            if not passage.strip():
                continue

            try:
                inputs = tokenizer(passage, return_tensors="pt", truncation=True, max_length=512).to(device)
                
                with torch.no_grad():
                    outputs = model(**inputs, labels=inputs['input_ids'])
                    loss = outputs.loss.item()
                    
                    # Guard-clause Ä‘á»ƒ trÃ¡nh lá»—i math domain error náº¿u loss khÃ´ng dÆ°Æ¡ng
                    if loss > 0:
                        perplexity = torch.exp(torch.tensor(loss)).item()
                        perplexities_for_this_category.append(perplexity)

            except Exception as e:
                print(f"      [Warning] Could not calculate perplexity for a passage in '{category_name}': {e}")

        # TÃ­nh toÃ¡n káº¿t quáº£ cho danh má»¥c hiá»‡n táº¡i
        if perplexities_for_this_category:
            avg_ppl = np.mean(perplexities_for_this_category)
            category_results[category_name] = {
                'average_perplexity': round(avg_ppl, 4),
                'num_passages': len(perplexities_for_this_category) # Chá»‰ Ä‘áº¿m nhá»¯ng passage Ä‘Ã£ tÃ­nh Ä‘Æ°á»£c
            }
            # ThÃªm táº¥t cáº£ Ä‘iá»ƒm PPL cá»§a danh má»¥c nÃ y vÃ o danh sÃ¡ch tá»•ng
            all_perplexities.extend(perplexities_for_this_category)
            print(f"    ğŸ“Š Category '{category_name}' Average PPL: {avg_ppl:.4f}")
        else:
            print(f"    âš ï¸ No valid perplexity scores calculated for category '{category_name}'.")

    # TÃ­nh toÃ¡n káº¿t quáº£ tá»•ng thá»ƒ tá»« táº¥t cáº£ cÃ¡c Ä‘iá»ƒm PPL Ä‘Ã£ thu tháº­p
    overall_avg_ppl = np.mean(all_perplexities) if all_perplexities else 0.0
    
    print(f"\n    ğŸ“ˆ Overall Average Perplexity across all categories: {overall_avg_ppl:.4f}")

    return {
        'task': 'Perplexity Test',
        'overall_average_perplexity': round(overall_avg_ppl, 4),
        'category_results': category_results
    }