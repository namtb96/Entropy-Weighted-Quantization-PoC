# ewq_benchmark/suite.py

import torch
import psutil
import time
import numpy as np
import json
from pathlib import Path
from datetime import datetime
import gc
from typing import Dict, List

from . import tasks

class EnhancedBenchmarkSuite:
    def __init__(self, model, tokenizer, model_id: str, run_name: str, model_hash: str):
        """
        Khá»Ÿi táº¡o bá»™ benchmark.

        Args:
            model: Model ngÃ´n ngá»¯.
            tokenizer: Tokenizer tÆ°Æ¡ng á»©ng.
            model_id (str): ID cá»§a model (vÃ­ dá»¥: 'Qwen/Qwen3-8B').
            run_name (str): TÃªn mÃ´ táº£ cho láº§n cháº¡y nÃ y (vÃ­ dá»¥: 'ewq-bitsandbytes' hoáº·c 'original-fp16').
            model_hash (str): Hash duy nháº¥t cá»§a cáº¥u hÃ¬nh model.
        """
        self.model = model
        self.tokenizer = tokenizer
        self.model_id = model_id
        self.run_name = run_name
        self.model_hash = model_hash
        self.device = next(model.parameters()).device

    def _get_memory_usage(self) -> Dict[str, float]:
        process = psutil.Process(); ram_gb = process.memory_info().rss / 1024**3
        gpu_allocated_gb = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        return {'ram_gb': ram_gb, 'gpu_allocated_gb': gpu_allocated_gb}

    def _generate_response(self, prompt: str, task_name: str = "") -> Dict:
      print(f"    ğŸ”„ Generating response for: {task_name}")
      
      inputs = self.tokenizer(
          prompt, 
          return_tensors="pt", 
          truncation=True, 
          max_length=4096
      ).to(self.device)
      
      start_time = time.time()
      
      with torch.no_grad():
          outputs = self.model.generate(
            **inputs,
            max_new_tokens=4096, 
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
          )
      
      end_time = time.time()
      
      input_length = inputs['input_ids'].shape[1]
      response_tokens = outputs[0][input_length:]
      
      # <<< THÃŠM BÆ¯á»šC Xá»¬ LÃ Háº¬U Ká»² á» ÄÃ‚Y >>>
      # Decode toÃ n bá»™ chuá»—i Ä‘á»ƒ tÃ¬m token káº¿t thÃºc dÆ°á»›i dáº¡ng text
      full_response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=False)
      
      # Láº¥y token káº¿t thÃºc tá»« tokenizer
      eos_token_text = self.tokenizer.decode(self.tokenizer.eos_token_id)

      # Cáº¯t chuá»—i táº¡i vá»‹ trÃ­ cá»§a token káº¿t thÃºc Ä‘áº§u tiÃªn
      if eos_token_text in full_response_text:
          clean_response_text = full_response_text.split(eos_token_text)[0]
      else:
          clean_response_text = full_response_text

      # Decode láº¡i láº§n ná»¯a vá»›i skip_special_tokens=True Ä‘á»ƒ dá»n dáº¹p
      final_response = self.tokenizer.decode(self.tokenizer.encode(clean_response_text), skip_special_tokens=True).strip()

      generation_time = end_time - start_time
      tokens_generated = len(response_tokens) # Váº«n Ä‘o lÆ°á»ng tá»•ng sá»‘ token Ä‘Æ°á»£c sinh ra
      tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
      
      return {
          'prompt': prompt,
          'response': final_response, # Tráº£ vá» response Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch
          'full_raw_response': full_response_text, # CÃ³ thá»ƒ giá»¯ láº¡i Ä‘á»ƒ debug
          'generation_time': round(generation_time, 2),
          'tokens_generated': tokens_generated,
          'tokens_per_second': round(tokens_per_second, 2),
          'vram_usage_gb': round(self._get_memory_usage()['gpu_allocated_gb'], 2)
      }


    def _run_single_generation_task(self, task_name: str, prompts: List[str]) -> Dict:
        print(f"  ğŸ“Š Benchmarking Generation Task: {task_name}")
        results = [self._generate_response(p, f"{task_name} #{i+1}") for i, p in enumerate(prompts)]
        avg_tps = np.mean([r['tokens_per_second'] for r in results])
        return {'task': task_name, 'avg_tokens_per_second': round(avg_tps, 2), 'tests': results}

    def run_full_benchmark(self, mmlu_dir: Path, PERPLEXITY_CATEGORIES: list, traditional_tasks: dict):
        """Cháº¡y toÃ n bá»™ bá»™ benchmark, nháº­n cÃ¡c test case lÃ m tham sá»‘."""
        print("\nğŸš€ Starting EWQ Model Comprehensive Benchmark")
        print("=" * 80)

        # Gá»i cÃ¡c hÃ m task tá»« module tasks.py
        mmlu_results = tasks.run_mmlu_test(self.model, self.tokenizer, self.device, mmlu_dir)
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        perplexity_results = tasks.run_perplexity_test(self.model, self.tokenizer, self.device, PERPLEXITY_CATEGORIES)
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        generation_results = []
        for name, prompts in traditional_tasks.items():
            generation_results.append(self._run_single_generation_task(name, prompts))
            gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None

        overall_stats = {
            'avg_token_speed': round(np.mean([r['avg_tokens_per_second'] for r in generation_results]), 2) if generation_results else 0,
            'mmlu_accuracy': mmlu_results.get('overall_accuracy', 0),
            'avg_perplexity': perplexity_results.get('overall_average_perplexity', 0)
        }
        
        full_results = {
            'model_hash': self.model_hash,
            'timestamp': datetime.now().isoformat(),
            'overall_stats': overall_stats,
            'mmlu_results': mmlu_results,
            'perplexity_results': perplexity_results,
            'generation_results': generation_results
        }
        self.save_and_print_summary(full_results)
        return full_results
    
    def save_and_print_summary(self, results: Dict):
        """LÆ°u vÃ  in káº¿t quáº£ benchmark vá»›i tÃªn file dá»… Ä‘á»c."""
        stats = results['overall_stats']
        
        # In ra báº£n tÃ³m táº¯t káº¿t quáº£
        print("\n" + "="*80 + "\nğŸ“Š EWQ MODEL ENHANCED BENCHMARK RESULTS SUMMARY\n" + "="*80)
        print(f"ğŸ“¦ Model ID: {self.model_id}")
        print(f"âš™ï¸ Run Type: {self.run_name}") # ThÃªm thÃ´ng tin loáº¡i run
        print(f"ğŸ¯ Model Hash: {results['model_hash']}") # Váº«n giá»¯ hash trong log Ä‘á»ƒ Ä‘á»‹nh danh
        print("-" * 80)
        print(f"ğŸ“ˆ Average Token Speed (Generation): {stats['avg_token_speed']:.2f} tokens/sec")
        print(f"ğŸ§  MMLU Accuracy: {stats['mmlu_accuracy']:.2f}%")
        print(f"ğŸ“Š Overall Average Perplexity: {stats['avg_perplexity']:.4f}")
        
        # In chi tiáº¿t káº¿t quáº£ perplexity theo danh má»¥c
        if 'category_results' in results['perplexity_results']:
            print("\n   --- Perplexity by Category ---")
            for category, data in results['perplexity_results']['category_results'].items():
                print(f"   - {category:<25}: {data['average_perplexity']:.4f}")
        
        # Táº¡o tÃªn file dá»… Ä‘á»c
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # LÃ m sáº¡ch tÃªn model Ä‘á»ƒ dÃ¹ng trong tÃªn file (loáº¡i bá» dáº¥u gáº¡ch chÃ©o)
        safe_model_name = self.model_id.split('/')[-1]
        
        # Táº¡o tÃªn file má»›i rÃµ rÃ ng hÆ¡n
        filename = f"benchmark_{safe_model_name}_{self.run_name}_{timestamp}.json"
        filepath = Path("benchmark_results") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            # ThÃªm run_name vÃ o ná»™i dung file JSON Ä‘á»ƒ tham kháº£o
            results['run_name'] = self.run_name
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        print(f"\nğŸ’¾ Full detailed results saved to: {filepath}")